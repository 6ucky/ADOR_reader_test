{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a1656c-50e7-4109-afd5-933723aae077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index==0.11.10\n",
      "  Using cached llama_index-0.11.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.1 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.10 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_core-0.11.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.3 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_llms_openai-0.2.9-py3-none-any.whl.metadata (648 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index==0.11.10)\n",
      "  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index==0.11.10)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.1->llama-index==0.11.10)\n",
      "  Downloading openai-1.50.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (0.6.6)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (2023.6.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (3.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (10.3.0)\n",
      "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (8.3.0)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10)\n",
      "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.14.1)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.10)\n",
      "  Using cached llama_cloud-0.1.0-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.10) (2.1.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.10) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.10)\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.10)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.10)\n",
      "  Using cached llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index==0.11.10) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index==0.11.10) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index==0.11.10) (2024.5.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.10) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index==0.11.10) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index==0.11.10)\n",
      "  Using cached jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (0.6.0)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10)\n",
      "  Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.10) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.10) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.10) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.10) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.10) (1.16.0)\n",
      "Using cached llama_index-0.11.10-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_core-0.11.14-py3-none-any.whl (1.6 MB)\n",
      "Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "Using cached llama_index_llms_openai-0.2.9-py3-none-any.whl (12 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.1.0-py3-none-any.whl (176 kB)\n",
      "Using cached llama_parse-0.5.6-py3-none-any.whl (10 kB)\n",
      "Downloading openai-1.50.1-py3-none-any.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.9/378.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, pypdf, pydantic-core, nltk, jiter, deprecated, tiktoken, pydantic, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.1\n",
      "    Uninstalling pydantic_core-2.18.1:\n",
      "      Successfully uninstalled pydantic_core-2.18.1\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.14\n",
      "    Uninstalling pydantic-1.10.14:\n",
      "      Successfully uninstalled pydantic-1.10.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "gluonts 0.13.7 requires pydantic~=1.7, but you have pydantic 2.9.2 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.15 requires pydantic==1.*, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deprecated-1.2.14 dirtyjson-1.0.8 jiter-0.5.0 llama-cloud-0.1.0 llama-index-0.11.10 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.14 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.9 llama-index-multi-modal-llms-openai-0.2.1 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.6 nltk-3.9.1 openai-1.50.1 pydantic-2.7.0 pydantic-core-2.23.4 pypdf-4.3.1 striprtf-0.0.26 tiktoken-0.7.0\n",
      "Collecting llama-index-llms-bedrock==0.2.1\n",
      "  Using cached llama_index_llms_bedrock-0.2.1-py3-none-any.whl.metadata (738 bytes)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.26 in /opt/conda/lib/python3.10/site-packages (from llama-index-llms-bedrock==0.2.1) (1.34.51)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-llms-bedrock==0.2.1) (0.11.14)\n",
      "Collecting llama-index-llms-anthropic<0.3.0,>=0.2.0 (from llama-index-llms-bedrock==0.2.1)\n",
      "  Using cached llama_index_llms_anthropic-0.2.1-py3-none-any.whl.metadata (638 bytes)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock==0.2.1) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock==0.2.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock==0.2.1) (0.10.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (2023.6.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (2.7.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.14.1)\n",
      "Collecting anthropic<0.29.0,>=0.26.2 (from llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1)\n",
      "  Using cached anthropic-0.28.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (4.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (0.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.51->boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock==0.2.1) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.51->boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock==0.2.1) (1.26.18)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (2024.5.10)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (0.6.0)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1)\n",
      "  Using cached pydantic_core-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (3.21.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-bedrock==0.2.1) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.51->boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock==0.2.1) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (0.23.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.29.0,>=0.26.2->llama-index-llms-anthropic<0.3.0,>=0.2.0->llama-index-llms-bedrock==0.2.1) (3.14.0)\n",
      "Using cached llama_index_llms_bedrock-0.2.1-py3-none-any.whl (8.5 kB)\n",
      "Using cached llama_index_llms_anthropic-0.2.1-py3-none-any.whl (6.6 kB)\n",
      "Using cached anthropic-0.28.1-py3-none-any.whl (862 kB)\n",
      "Using cached pydantic_core-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Installing collected packages: pydantic-core, anthropic, llama-index-llms-anthropic, llama-index-llms-bedrock\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-jupyterlab-extension-common 0.1.15 requires pydantic==1.*, but you have pydantic 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.28.1 llama-index-llms-anthropic-0.2.1 llama-index-llms-bedrock-0.2.1 pydantic-core-2.18.1\n",
      "Collecting llama-index-embeddings-bedrock==0.3.1\n",
      "  Using cached llama_index_embeddings_bedrock-0.3.1-py3-none-any.whl.metadata (697 bytes)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.23 in /opt/conda/lib/python3.10/site-packages (from llama-index-embeddings-bedrock==0.3.1) (1.34.51)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-embeddings-bedrock==0.3.1) (0.11.14)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.23->llama-index-embeddings-bedrock==0.3.1) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.23->llama-index-embeddings-bedrock==0.3.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.23->llama-index-embeddings-bedrock==0.3.1) (0.10.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (2023.6.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (2.7.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.14.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (4.0.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.51->boto3<2.0.0,>=1.34.23->llama-index-embeddings-bedrock==0.3.1) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.51->boto3<2.0.0,>=1.34.23->llama-index-embeddings-bedrock==0.3.1) (1.26.18)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (2024.5.10)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (3.21.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.51->boto3<2.0.0,>=1.34.23->llama-index-embeddings-bedrock==0.3.1) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-bedrock==0.3.1) (1.2.0)\n",
      "Using cached llama_index_embeddings_bedrock-0.3.1-py3-none-any.whl (5.5 kB)\n",
      "Installing collected packages: llama-index-embeddings-bedrock\n",
      "Successfully installed llama-index-embeddings-bedrock-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index==0.11.10\n",
    "!pip install llama-index-llms-bedrock==0.2.1\n",
    "!pip install llama-index-embeddings-bedrock==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a49510-1acf-480a-98ce-13416adce28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.core import ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser.text.sentence import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4b1f9c-6f13-4595-9bbd-0d5d4449a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.indices.utils import (\n",
    "    default_format_node_batch_fn,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.output_parsers.pydantic import PydanticOutputParser\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional, Type, Union\n",
    "\n",
    "SUB_QUESTION_PROMPT_STR = \"\"\"\n",
    "Vous êtes un assistant utile qui génère plusieurs requêtes de recherche basées sur une seule requête d'entrée. \n",
    "Générer 2 requêtes de recherche, une sur chaque ligne, liées à la requête d'entrée suivante:\n",
    "Requête: {query}\n",
    "Requêtes:\n",
    "\"\"\"\n",
    "\n",
    "PYDANTIC_FORMAT_TMPL = \"\"\"\n",
    "Voici un schéma JSON à suivre:\n",
    "{schema}\n",
    "\n",
    "Générez un objet JSON valide mais ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_SELECT_PROMPT_TMPL = \"\"\"\n",
    "Une liste de documents est présentée ci-dessous. Chaque document est accompagné d'un numéro et d'un résumé du document. Une question est également fournie.\n",
    "Donne-moi uniquement les numéros des documents pertinents à la question en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "...\n",
    "\n",
    "Document 10:\n",
    "<résumé du document 10>\n",
    "\n",
    "Question: <question>\n",
    "Tableau des nombres des documents pertinents: [2, 4, 5, 6]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "{context_str}\n",
    "Question: {query_str}\n",
    "Tableau des nombres des documents pertinents:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents pertinents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_PROMPT_TMPL = \"\"\"\n",
    "Voici une liste de contextes pertinents pour une requête donnée. Vous devez filtrer cette liste pour exclure les contextes qui correspondent à un contexte négatif spécifique.\n",
    "Donne-moi uniquement les numéros des documents en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Requête principale : <question>\n",
    "Contexte négatif : <contexte_negatif>\n",
    "Liste des contextes :\n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "Document 3:\n",
    "<résumé du document 10>\n",
    "\n",
    "Document 4:\n",
    "<résumé du document 10>\n",
    "\n",
    "Tableau des nombres des documents: [1, 2, 3]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "Requête principale : {query_str}\n",
    "Contexte négatif : {contexte_negatif}\n",
    "Liste des contextes : {contexte}\n",
    "Tableau des nombres des documents:\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "class DocumentNumberInfo(BaseModel):\n",
    "    \"\"\"Informations concernant un tableau structuré.\"\"\"\n",
    "    tableau_document: list = Field(\n",
    "        ..., description=\"le tableau des nombres des documents\"\n",
    "    )\n",
    "\n",
    "class SubQuestionEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "\n",
    "class RerankerValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerBatchEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "    batch_nodes: list\n",
    "\n",
    "class RerankerValidationDone(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerFlow(Workflow):\n",
    "    def __init__(self, vector_retriever: VectorIndexRetriever, timeout = 120, verbose = True):\n",
    "        self.llm = Bedrock(model=\"anthropic.claude-3-sonnet-20240229-v1:0\", region_name=\"eu-west-3\")\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.batch_size = 3 # the divided size to rerank all the nodes \n",
    "        self.max_retries = 3 # the max retries for the prompt\n",
    "        self.min_chunk = 10 # the minimum size of the chunks after the rerank\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "\n",
    "    # the state to generate sub questions and put inside nodes\n",
    "    @step()\n",
    "    async def sub_question(\n",
    "        self, ev: StartEvent\n",
    "    ) -> SubQuestionEvent:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        # subquestions = []\n",
    "        # try:\n",
    "        #     subquestions_response = Settings.llm.complete(SUB_QUESTION_PROMPT_STR.format(query=query))\n",
    "        #     subquestions = subquestions_response.text.split(\"\\n\")\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        # if len(subquestions) > 2:\n",
    "        #     nodes1 = self.vector_retriever.retrieve(subquestions[-2])\n",
    "        #     nodes2 = self.vector_retriever.retrieve(subquestions[-1])\n",
    "        #     one_third_index = int(len(nodes)/3)\n",
    "        #     two_third_index = int(len(nodes)/3*2)\n",
    "        #     nodes_ids = []\n",
    "        #     for node in nodes:\n",
    "        #         nodes_ids.append(node.id_)\n",
    "        #     indicator1 = 0\n",
    "        #     indicator2 = 0\n",
    "        #     for i in range(len(nodes1)):\n",
    "        #         if nodes1[i].id_ not in nodes_ids:\n",
    "        #             indicator1 += 1\n",
    "        #             nodes_ids.append(nodes1[i].id_)\n",
    "        #             if indicator1 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 5:\n",
    "        #                 nodes.append(nodes1[i])\n",
    "        #             elif indicator1 == 6:\n",
    "        #                 nodes.append(nodes1[i])\n",
    "        #                 break\n",
    "        #     for i in range(len(nodes2)):\n",
    "        #         if nodes2[i].id_ not in nodes_ids:\n",
    "        #             indicator2 += 1\n",
    "        #             nodes_ids.append(nodes2[i].id_)\n",
    "        #             if indicator2 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 5:\n",
    "        #                 nodes.append(nodes2[i])\n",
    "        #             elif indicator2 == 6:\n",
    "        #                 nodes.append(nodes2[i])\n",
    "        #                 break\n",
    "        return SubQuestionEvent(query=query, neg_context=neg_context, nodes=nodes)\n",
    "\n",
    "    # the state to generate prompt for reranking\n",
    "    @step(pass_context=True)\n",
    "    async def reranker_prompt(\n",
    "        self, ctx: Context, ev: Union[SubQuestionEvent, RerankerValidationErrorEvent, RerankerBatchEvent]\n",
    "    ) -> Union[StopEvent, RerankerDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        window = int(len(nodes)/3)\n",
    "\n",
    "        if isinstance(ev, SubQuestionEvent):\n",
    "            rerank_nodes = []\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, RerankerBatchEvent):\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            # iterate the nodes based on batch size\n",
    "            if current_batch >= self.batch_size:\n",
    "                print(\"Reranked nodes are extracted.\")\n",
    "                # if no negative context, output reranked nodes. otherwise, go the state of filter\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        # prompt error event\n",
    "        elif isinstance(ev, RerankerValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            if current_retries >= self.max_retries:\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = CHOICE_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        \n",
    "        prompt = CHOICE_SELECT_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return RerankerDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes, batch_nodes=batch_nodes)\n",
    "\n",
    "    # call LLM to rerank nodes\n",
    "    @step()\n",
    "    async def rerank_validate(\n",
    "        self, ev: RerankerDone\n",
    "    ) -> Union[StopEvent, RerankerValidationDone, RerankerValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            choice_output_parser = PydanticOutputParser(output_cls=DocumentNumberInfo, pydantic_format_tmpl=PYDANTIC_FORMAT_TMPL)\n",
    "            choice_program = LLMTextCompletionProgram.from_defaults(\n",
    "                output_parser=choice_output_parser,\n",
    "                llm=self.llm,\n",
    "                prompt_template_str=ev.prompt,\n",
    "            )\n",
    "            output = choice_program(context_str=default_format_node_batch_fn(ev.batch_nodes), query_str=ev.query)\n",
    "            new_nodes = []\n",
    "            for i in output.tableau_document:\n",
    "                if i-1 < len(ev.batch_nodes):\n",
    "                    new_nodes.append(ev.batch_nodes[i-1])\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return RerankerValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        rerank_nodes = new_nodes + ev.rerank_nodes\n",
    "        # if reranked nodes are less than min chunks, return to the state of batch event, otherwise output the reranked nodes.\n",
    "        if len(rerank_nodes) > self.min_chunk:\n",
    "            print(\"Reranked nodes are extracted.\")\n",
    "            if ev.neg_context == '':\n",
    "                return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                return RerankerValidationDone(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "        else:\n",
    "            return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # prepare the prompt to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_prompt(\n",
    "        self, ctx: Context, ev: Union[RerankerValidationDone, FilterValidationErrorEvent]\n",
    "    ) -> Union[StopEvent, FilterDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        rerank_nodes = ev.rerank_nodes\n",
    "\n",
    "        if isinstance(ev, RerankerValidationDone):\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, FilterValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            if current_retries >= self.max_retries:\n",
    "                if len(rerank_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            reflection_prompt = NEGATIVE_FILTER_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        prompt = NEGATIVE_FILTER_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return FilterDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # call LLM to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_validate(\n",
    "        self, ctx: Context, ev: FilterDone\n",
    "    ) -> Union[StopEvent, FilterValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            choice_output_parser = PydanticOutputParser(output_cls=DocumentNumberInfo, pydantic_format_tmpl=PYDANTIC_FORMAT_TMPL)\n",
    "            filter_program = LLMTextCompletionProgram.from_defaults(\n",
    "                output_parser=choice_output_parser,\n",
    "                llm=self.llm,\n",
    "                prompt_template_str=ev.prompt,\n",
    "            )\n",
    "            output = filter_program(context_str=default_format_node_batch_fn(ev.rerank_nodes), query_str=ev.query, contexte_negatif=ev.neg_context)\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return FilterValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        new_nodes = []\n",
    "        for i in output.tableau_document:\n",
    "            new_nodes.append(ev.rerank_nodes[i-1])\n",
    "        print(\"Reranked nodes are filtered.\")\n",
    "        current_batch = ctx.data.get(\"batch\", 0)\n",
    "        if len(new_nodes) > self.min_chunk:\n",
    "            return StopEvent(result=new_nodes)\n",
    "        else:\n",
    "            if current_batch >= self.batch_size:\n",
    "                if len(new_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=new_nodes)\n",
    "            else:\n",
    "                return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e41f63-8114-4707-bd21-feac08b2edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from llama_index.core.callbacks.base import CallbackManager\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    ")\n",
    "from llama_index.core.indices.keyword_table.base import BaseKeywordTableIndex\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "\n",
    "class KeywordTableLexiqueRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Extracts keywords from lexique using space separator.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: BaseKeywordTableIndex,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        object_map: Optional[dict] = None,\n",
    "        verbose: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self.index = index\n",
    "        self.lexique_keywords = self._get_lexique_keywords()\n",
    "        super().__init__(\n",
    "            callback_manager=callback_manager,\n",
    "            object_map=object_map,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    def _get_lexique_keywords(self):\n",
    "        lexique_keywords = []\n",
    "        for item in self.index.docstore.docs.values():\n",
    "            if \"keyword\" in item.metadata:\n",
    "                lexique_keywords.append(item.metadata[\"keyword\"])\n",
    "        return lexique_keywords\n",
    "\n",
    "    def _get_keywords(self, query_str: str) -> List[str]:\n",
    "        words = query_str.split()\n",
    "        keywords = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.lexique_keywords and word not in keywords:\n",
    "                keywords.append(word)\n",
    "            elif word[:-1] in self.lexique_keywords and word[:-1] not in keywords:\n",
    "                keywords.append(word[:-1])\n",
    "            elif word[1:] in self.lexique_keywords and word[1:] not in keywords:\n",
    "                keywords.append(word[1:])\n",
    "        return keywords\n",
    "\n",
    "    def _retrieve(\n",
    "        self,\n",
    "        query_bundle: QueryBundle,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Get nodes for response.\"\"\"\n",
    "        \n",
    "        keywords = self._get_keywords(query_bundle.query_str)\n",
    "        print(f\"query keywords: {keywords}\")\n",
    "\n",
    "        sorted_nodes = []\n",
    "        for node in self.index.docstore.docs.values():\n",
    "            if \"keyword\" in node.metadata and node.metadata[\"keyword\"] in keywords:\n",
    "                sorted_nodes.append(node)\n",
    "\n",
    "        return [NodeWithScore(node=node) for node in sorted_nodes]\n",
    "    \n",
    "    async def _aretrieve(\n",
    "        self,\n",
    "        query_bundle: QueryBundle,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Get nodes for response.\"\"\"\n",
    "        \n",
    "        keywords = self._get_keywords(query_bundle.query_str)\n",
    "        print(f\"query keywords: {keywords}\")\n",
    "\n",
    "        sorted_nodes = []\n",
    "        for node in self.index.docstore.docs.values():\n",
    "            if \"keyword\" in node.metadata and node.metadata[\"keyword\"] in keywords:\n",
    "                sorted_nodes.append(node)\n",
    "\n",
    "        return [NodeWithScore(node=node) for node in sorted_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4711f1-2ec5-4249-912d-ca3d5673418a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.indices.keyword_table import KeywordTableGPTRetriever\n",
    "from typing import List\n",
    "import threading\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableGPTRetriever,\n",
    "        embed_model: BedrockEmbedding,\n",
    "        old_queries: List = [],\n",
    "        mode: str = \"OR\",\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self.embed_model = embed_model\n",
    "        self.reranker = RerankerFlow(vector_retriever=vector_retriever)\n",
    "        self.old_queries = old_queries\n",
    "        index_feedback = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"finetune/vector_persist_800_feedback\"))\n",
    "        self.feedback_retriever = VectorIndexRetriever(index=index_feedback, similarity_top_k=1)\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async not activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = self._weighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = await self._aweighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        feedback_nodes = self.feedback_retriever.retrieve(query_bundle)\n",
    "        if feedback_nodes[0].score > 0.8 : \n",
    "            neg_context = feedback_nodes[0].metadata[\"message\"]\n",
    "        else:\n",
    "            neg_context = ''\n",
    "        vector_nodes = await self.reranker.run(query=query_bundle.query_str, nodes=vector_nodes_full, neg_context=neg_context)\n",
    "        if vector_nodes == None:\n",
    "            return keyword_nodes\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    def replace_abbreviations(self, question, keyword_nodes):\n",
    "        \"\"\"\n",
    "        Replaces abbreviations in the lexique with their complete names.\n",
    "        Assumes abbreviations are separated by either two spaces or one space + one punctuation mark.\n",
    "        \"\"\"\n",
    "        lexique = {}\n",
    "        for node in keyword_nodes:\n",
    "            keyword = node.node.metadata['keyword']\n",
    "            keyword_complete = node.text.split(': ')[1]\n",
    "            lexique[keyword] = keyword_complete\n",
    "\n",
    "        words = question.split()\n",
    "        result = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in lexique:\n",
    "                result.append(lexique[word])\n",
    "                result.append('('+word+')')\n",
    "            elif word[:-1] in lexique:\n",
    "                result.append(lexique[word[:-1]])\n",
    "                result.append('('+word[:-1]+')')\n",
    "                result.append(word[-1])\n",
    "            elif word[1:] in lexique:\n",
    "                result.append(word[0])\n",
    "                result.append(lexique[word[1:]])\n",
    "                result.append('('+word[1:]+')')\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def _get_query_embedding_threaded(self, query):\n",
    "        \"\"\"\n",
    "        Get the query embedding using a separate thread.\n",
    "        \"\"\"\n",
    "        result_queue = Queue()  # Create a queue to store the result\n",
    "        thread = threading.Thread(target=self._get_query_embedding_async_thread, args=(query, result_queue))\n",
    "        thread.start()\n",
    "        thread.join()  # Wait for the thread to finish\n",
    "        return result_queue.get()  # Retrieve the result from the queue\n",
    "\n",
    "    def _get_query_embedding_async_thread(self, query, result_queue):\n",
    "        \"\"\"\n",
    "        Helper method to run the asynchronous call in a separate thread.\n",
    "        \"\"\"\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(self.embed_model.aget_query_embedding(query))\n",
    "        loop.close()\n",
    "        result_queue.put(result)  # Put the result in the queue\n",
    "    \n",
    "    def _weighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = self._get_query_embedding_threaded(new_query_str)\n",
    "        print(current_embedding)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(self._get_query_embedding_threaded(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding\n",
    "    async def _aweighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = await self.embed_model.aget_query_embedding(new_query_str)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(await self.embed_model.aget_query_embedding(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435a4471-5a77-4950-92e6-5369c85ee428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def display_nodes(nodes):\n",
    "    nodes_output = ''\n",
    "    for node in nodes:\n",
    "        nodes_output += 'ID: ' + node.id_ + '\\n'\n",
    "        if 'page_label' in node.metadata:\n",
    "            nodes_output += 'Pagelabel: ' + node.metadata['page_label'] + '\\n'\n",
    "        else:\n",
    "            pass\n",
    "        if 'file_name' in node.metadata:\n",
    "            nodes_output += 'File name: ' + node.metadata['file_name'] + '\\n'\n",
    "        else:\n",
    "            pass\n",
    "        if 'file_path' in node.metadata:\n",
    "            nodes_output += 'File path: ' + node.metadata['file_path'] + '\\n'\n",
    "        else:\n",
    "            pass\n",
    "        if 'metier' in node.metadata:\n",
    "            nodes_output += 'Metier: ' + str(node.metadata['metier']) + '\\n'\n",
    "        else:\n",
    "            pass\n",
    "        nodes_output += 'Score: ' + str(node.score) + '\\n'\n",
    "        nodes_output += 'Text: ' + str(node.text) + '\\n'\n",
    "        nodes_output += '################\\n'\n",
    "    return nodes_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe49b7d-078d-4b7b-b9b6-6537d57069b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_QUERY_PROMPT_TMPL = (\n",
    "    \" Les informations contextuelles sont\"\n",
    "    \" ci-dessous.\\n---------------------\\n{context_str}\\n---------------------\\n\"\n",
    "    \" Compte tenu des informations contextuelles, répondez à la requête sans reproduire la requète.\\n\"\n",
    "    \" Baser la réponse sur le contexte fourni, sans mentionner les noms des documents sources. Structurer la réponse comme suit :\\n\"\n",
    "    \" - Formater le texte avec une ponctuation appropriés entre les phrases.\\n\"\n",
    "    \" - Lorsqu'il y a une enumeration d'actions ou d'étapes dans la réponse, les précéder d'un tiret et les numéroter (1-, 2-, etc.)\\n\"\n",
    "    \" - Commencer chaque nouvelle phrase ou action par une majuscule.\\n\"\n",
    "    \" - Relire la réponse pour s'assurer qu'elle est claire, cohérente et bien formatée avant de la soumettre.\"\n",
    "    \" Requête: {query_str}\\n\"\n",
    "    \" Réponse:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290ca903-fa20-4650-99a1-026a9ee5621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n",
    "    \"Un texte est fourni ci-dessous. Étant donné le texte, extrayez jusqu'à {max_keywords} \"\n",
    "    \"keywords du texte. Évitez les stopwords.\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Fournissez des keywords au format suivant: 'KEYWORDS: <keywords>'\\n\"\n",
    ")\n",
    "KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n",
    "    KEYWORD_EXTRACT_TEMPLATE_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9aed20-72a1-4f19-ae5d-e24a5e30aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session = boto3.session.Session()\n",
    "region_name = \"eu-central-1\"\n",
    "bedrock_agent_client = boto3_session.client('bedrock-runtime', region_name=region_name)\n",
    "Settings.embed_model = BedrockEmbedding(client=bedrock_agent_client, model_name=\"amazon.titan-embed-text-v1\")\n",
    "Settings.llm = Bedrock(model=\"anthropic.claude-3-haiku-20240307-v1:0\", region_name=\"eu-west-3\")\n",
    "chunk_size = 800\n",
    "Settings.node_parser = SentenceSplitter.from_defaults(chunk_size=chunk_size, chunk_overlap=50)\n",
    "Settings.num_output = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673222f9-e96d-48b1-8ea1-46073b6b47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"vector_persist_800_main\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34fd614-5c89-46ff-b60c-ff0cc182fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26482\n"
     ]
    }
   ],
   "source": [
    "nodes = index_doc.docstore.docs.values()\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d6399d-e143-4c97-b4a1-adfd9e5986eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_keyword = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"vector_persist_800_lexique\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edd9f267-7af5-4d2f-bbac-58890a5aaa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index_doc, similarity_top_k=30)\n",
    "keyword_retriever = KeywordTableGPTRetriever(index=index_keyword, max_keywords_per_query = 20, keyword_extract_template=KEYWORD_EXTRACT_TEMPLATE)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever, embed_model=Settings.embed_model, old_queries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88061b10-1047-45bc-82f9-8cd0a560926f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "14416d8b-d04e-463c-bde6-a4074dac106c\n",
      "0.6292323751243815\n",
      "#######\n",
      "8cb8e165-c7f5-474b-ab8f-533ce6ee563e\n",
      "0.6209355792525079\n",
      "#######\n",
      "ab038de4-5ce8-414b-b927-c5576cc0a8e4\n",
      "0.6143688385279331\n",
      "#######\n",
      "4a955058-7ddc-4e34-9017-1e89994f927e\n",
      "0.6304759656740551\n",
      "#######\n",
      "28e895b0-4337-45f5-b1ef-283df9c63e0c\n",
      "0.6583704601825731\n",
      "#######\n",
      "756a78d7-ab8a-4964-9b1d-977e926a3ddc\n",
      "0.6118743166167164\n",
      "#######\n",
      "29dc8db9-0d16-428b-89d9-ea64f9f0f574\n",
      "0.6160640018461343\n",
      "#######\n",
      "e7d7d37c-f5d0-4345-82e3-707c204f6141\n",
      "0.6770714333514298\n",
      "#######\n",
      "73c48700-4962-4396-ab2e-394ed9faae4b\n",
      "0.6522693976772027\n",
      "#######\n",
      "ec8c519d-2813-4e0f-9593-3ac3486199c0\n",
      "0.6705403457837324\n",
      "#######\n",
      "fbb862b2-045a-4d1e-931b-4910c34caffd\n",
      "0.6120163237056044\n",
      "#######\n",
      "8471c5fa-7fb0-4b30-93ff-71297c693945\n",
      "None\n",
      "#######\n",
      "32cd064c-92aa-4d51-bf44-633fc29b2179\n",
      "0.6191427544715852\n",
      "#######\n",
      "4db28a50-5f19-4ec9-bdef-224a7261a369\n",
      "0.6082360682767516\n",
      "#######\n",
      "16eb41df-f992-4085-8edc-04c65a22948d\n",
      "0.6954048925386471\n",
      "#######\n",
      "398b8bd7-e531-4282-905e-d09f3c6a763b\n",
      "0.6145571188927477\n",
      "#######\n",
      "a75c7d64-0cfd-4f53-9e90-10d08c7f1b3a\n",
      "0.6286860045231071\n",
      "#######\n",
      "6684a125-1cdb-40dd-9aec-d03f8632de23\n",
      "0.5212826325818306\n",
      "#######\n",
      "6aae187c-185f-46ba-8735-33f139711943\n",
      "0.6516611423374632\n",
      "#######\n",
      "807bafe9-f49b-412f-9e35-2f584152e3ae\n",
      "0.5983137995109185\n",
      "#######\n",
      "1271ac09-7359-4ab0-986a-c835772bfaf9\n",
      "0.6959471221085849\n",
      "#######\n",
      "Execution Time: 28.683359622955322 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "nodes = await custom_retriever.aretrieve(\"Que veut dire CICM?\")\n",
    "for node in nodes:\n",
    "    print(node.id_)\n",
    "    print(node.score)\n",
    "    print(\"#######\")\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3587ac-c49f-4de4-b5a6-1f8beb729002",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## old solution  -- 29.55 seconds / 4 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab90f3c5-8ddb-4a0e-874d-f5cdedfb0d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0.post104)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.40.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.10)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.2.2)\n",
      "Using cached sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97ac420c-3154-418e-b17a-6fbc0294ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-09-27 11:41:14.926133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableGPTRetriever,\n",
    "        embed_model: BedrockEmbedding,\n",
    "        old_queries: List = [],\n",
    "        mode: str = \"OR\",\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self.embed_model = embed_model\n",
    "        self.old_queries = old_queries\n",
    "        self.crossencoder_model = CrossEncoder('finetune/crossencoder-camembert-base-mmarcoFR')\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async not activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = self._weighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "        vector_nodes = self.rerank_retrieve(query_bundle,vector_nodes_full)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        for node in keyword_nodes:\n",
    "            print(node.id_)\n",
    "            print(node.score)\n",
    "            print(node.text)\n",
    "            print(\"#######\")\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = await self._aweighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "        vector_nodes = self.rerank_retrieve(query_bundle,vector_nodes_full)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    def rerank_retrieve(self, query_bundle: QueryBundle, vector_nodes: List[NodeWithScore]):\n",
    "        if len(vector_nodes) < 2:\n",
    "            return vector_nodes\n",
    "        pairs = []\n",
    "        for node in vector_nodes:\n",
    "            pairs.append((query_bundle.query_str, node.text))\n",
    "        scores = self.crossencoder_model.predict(pairs)\n",
    "\n",
    "        zipped = zip(scores, vector_nodes)\n",
    "        zipped = sorted(zipped, key=lambda x: x[0], reverse=True)\n",
    "        numbers, nodes = zip(*zipped)\n",
    "        return nodes[:int(0.7 * len(nodes))]\n",
    "\n",
    "    def replace_abbreviations(self, question, keyword_nodes):\n",
    "        \"\"\"\n",
    "        Replaces abbreviations in the lexique with their complete names.\n",
    "        Assumes abbreviations are separated by either two spaces or one space + one punctuation mark.\n",
    "        \"\"\"\n",
    "        lexique = {}\n",
    "        for node in keyword_nodes:\n",
    "            keyword = node.node.metadata['keyword']\n",
    "            keyword_complete = node.text.split(': ')[1]\n",
    "            lexique[keyword] = keyword_complete\n",
    "\n",
    "        words = question.split()\n",
    "        result = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in lexique:\n",
    "                result.append(lexique[word])\n",
    "                result.append('('+word+')')\n",
    "            elif word[:-1] in lexique:\n",
    "                result.append(lexique[word[:-1]])\n",
    "                result.append('('+word[:-1]+')')\n",
    "                result.append(word[-1])\n",
    "            elif word[1:] in lexique:\n",
    "                result.append(word[0])\n",
    "                result.append(lexique[word[1:]])\n",
    "                result.append('('+word[1:]+')')\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def _get_query_embedding_threaded(self, query):\n",
    "        \"\"\"\n",
    "        Get the query embedding using a separate thread.\n",
    "        \"\"\"\n",
    "        result_queue = Queue()  # Create a queue to store the result\n",
    "        thread = threading.Thread(target=self._get_query_embedding_async_thread, args=(query, result_queue))\n",
    "        thread.start()\n",
    "        thread.join()  # Wait for the thread to finish\n",
    "        return result_queue.get()  # Retrieve the result from the queue\n",
    "\n",
    "    def _get_query_embedding_async_thread(self, query, result_queue):\n",
    "        \"\"\"\n",
    "        Helper method to run the asynchronous call in a separate thread.\n",
    "        \"\"\"\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(self.embed_model.aget_query_embedding(query))\n",
    "        loop.close()\n",
    "        result_queue.put(result)  # Put the result in the queue\n",
    "    \n",
    "    def _weighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = self._get_query_embedding_threaded(new_query_str)\n",
    "        print(current_embedding)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(self._get_query_embedding_threaded(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding\n",
    "    async def _aweighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        print(\"async activated\")\n",
    "        current_embedding = await self.embed_model.aget_query_embedding(new_query_str)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(await self.embed_model.aget_query_embedding(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a087cba3-21c9-4d6a-a995-21948f1addc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index_doc, similarity_top_k=15)\n",
    "keyword_retriever = KeywordTableGPTRetriever(index=index_keyword, max_keywords_per_query = 20, keyword_extract_template=KEYWORD_EXTRACT_TEMPLATE)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever, embed_model=Settings.embed_model, old_queries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afec0f82-a790-4ed8-af09-280dfb30916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "qa_template = PromptTemplate(DEFAULT_QUERY_PROMPT_TMPL)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=custom_retriever,\n",
    "                                              text_qa_template=qa_template,\n",
    "                                              use_async=True,\n",
    "                                              response_mode = ResponseMode.SIMPLE_SUMMARIZE,\n",
    "                                              streaming=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecd0c0e4-17b2-41c9-a02d-5bdae8a9cc98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "a6d34e81-2270-41a0-bd53-29e9a97ff6d2\n",
      "None\n",
      "Ac : Acier joint Coulé\n",
      "#######\n",
      "1de44c9a-4dd1-4599-9903-764c2c365b2d\n",
      "None\n",
      "APE : Acier revetu Polyéthylène\n",
      "#######\n",
      "ebdb32c9-3c6f-4014-b615-c8a22564631b\n",
      "None\n",
      "A / Ac : Acier\n",
      "#######\n",
      "2442f36a-1370-4c42-b40b-a69da0d2eb48\n",
      "None\n",
      "Ast : Acier Joint Standard\n",
      "#######\n",
      "7ced680c-d34c-4f2f-8047-0446ff4493f1\n",
      "None\n",
      "DPBA : Dispositif Protection Branchement Acier\n",
      "#######\n",
      "a72f7bb9-fe08-44a9-95e4-77a962738124\n",
      "None\n",
      "Ac : Acier\n",
      "#######\n",
      "2ad7b4b6-788f-4894-aa1f-79796c324780\n",
      "None\n",
      "MBDI : Manchette de branchement à déclencheur intégré (réseau Acier)\n",
      "#######\n",
      "d78a7b5e-686e-4ed8-9f40-a22705584925\n",
      "None\n",
      "A : Acier joint soudé\n",
      "#######\n",
      "0cf43a38-1334-4ae0-9aa6-1eeaeb7deef7\n",
      "None\n",
      "Av : Acier joint vissé\n",
      "#######\n",
      "092d3436-9a5c-47d9-9f93-23d2aa74620c\n",
      "None\n",
      "AR : Acier revetu Brai\n",
      "#######\n",
      "async activated\n",
      "========================================\n",
      "Selon les informations contextuelles fournies, les tubes en acier doivent être assemblés soit par brasage capillaire (\"fort\" ou \"tendre\") pour les tubes de diamètre extérieur inférieur ou égal à 54 mm, soit par soudo-brasage pour les tubes de diamètre extérieur supérieur ou égal à 42 mm et inférieur ou égal à 110 mm. \n",
      "\n",
      "L'utilisation de la brasure tendre (température de fusion du métal d'apport inférieure à 450 °C) n'est autorisée que dans certains cas spécifiques, notamment pour les installations intérieures des habitations individuelles alimentées à une pression au plus égale à 400 mbar.\n",
      "\n",
      "Les assemblages par brasage capillaire doivent être réalisés exclusivement par raccords conformes à la spécification ATG B 524 ou, dans le cas d'éléments préfabriqués, par emboîture venue d'usine et répondant aux prescriptions correspondantes de la spécification ATG B 600.\n",
      "========================================\n",
      "Execution Time: 32.08571481704712 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca3048c8-cf2e-438d-adb7-df86d51ad464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "7cf600ba-afae-43f8-9801-b7fc04ebd0e8\n",
      "None\n",
      "OMA : Logiciel de Gestion des Marchés d' Achats\n",
      "#######\n",
      "704986ef-6ef3-4b95-95b0-e277d30e9d9c\n",
      "None\n",
      "E-consult : Portail Internet de mise à disposition des appels d’offre pour les fournisseurs (Métier Achats)\n",
      "#######\n",
      "cd932db3-47d6-4d7c-ab1c-5d0c920277a3\n",
      "None\n",
      "GGO : Gestionnaire des certificats de Garantie d'Origine biométhane\n",
      "#######\n",
      "0a6d9496-7a81-45f2-8179-cea51bf3117d\n",
      "None\n",
      "GO : (biométhane) Garantie d'Origine Le registre des GO enregistre les acteurs, les sites et les mouvements de GO\n",
      "#######\n",
      "ec88550b-3525-4ab4-8882-fd9939c55bff\n",
      "None\n",
      "PTF Bio : Plateforme Digital Biométhane\n",
      "#######\n",
      "8e9c6c1c-a8a7-4e62-b623-77419fcb0e82\n",
      "None\n",
      "CRAB : Compte-Rendu Annuel Biométhane\n",
      "#######\n",
      "async activated\n",
      "========================================\n",
      "Il n'y a pas d'informations spécifiques sur les achats immobilisés pour le biométhane dans le contexte fourni. Les documents traitent plutôt des principes généraux de fonctionnement et d'exploitation des postes d'injection de biométhane, ainsi que des aspects de maintenance et de contrôle de la qualité du biométhane injecté sur le réseau de distribution de gaz naturel.\n",
      "========================================\n",
      "Execution Time: 29.902750253677368 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quels sont les achats immobilisés pour le biométhane ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3741c8d6-80e3-4c32-bbc4-723cd2dfd76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "8471c5fa-7fb0-4b30-93ff-71297c693945\n",
      "None\n",
      "CICM : Conduite d'Immeuble, Conduite Montante\n",
      "#######\n",
      "async activated\n",
      "========================================\n",
      "CICM signifie \"Conduite d'Immeuble, Conduite Montante\". C'est un terme utilisé pour désigner les tuyauteries verticales (conduites montantes) et horizontales (conduites d'immeuble) qui alimentent en gaz les différents niveaux d'un bâtiment d'habitation collectif.\n",
      "========================================\n",
      "Execution Time: 27.767505645751953 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Que veut dire CICM ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae8899f3-3cff-4dd6-a437-c07840138f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "829d4a92-9fae-49f7-b58b-c4071df163bd\n",
      "None\n",
      "VSR : Véhicule de Surveillance des Réseaux\n",
      "#######\n",
      "828b100a-66f1-4187-b294-2799918bfceb\n",
      "None\n",
      "TARG : Téléchargement des Applications Réseaux Gaz.\n",
      "#######\n",
      "9b7ab968-6d93-4ffb-8e28-7d9219fb066a\n",
      "None\n",
      "ATLAS : Logiciel de Cartographie des Réseaux à Grande Echelle\n",
      "#######\n",
      "0b941bdb-afdd-4a9c-ba35-f7771ff06570\n",
      "None\n",
      "AIPR : Autorisation d’intervention à proximité des réseaux\n",
      "#######\n",
      "31de9c4b-5387-4da6-b9f2-d63c7db3ab17\n",
      "None\n",
      "GRHYD : Gestion des Réseaux par l'injection d'Hydrogène pour Décarboner les énergies. (ADEME+Engie)\n",
      "#######\n",
      "9da32904-5699-4675-9be4-c51e6e190faf\n",
      "None\n",
      "VGD : Voirie et Réseaux Divers\n",
      "#######\n",
      "19a8994b-bbfd-4799-be18-9e3332113ff4\n",
      "None\n",
      "PHARE : Portail Habilitations et Accès  aux Réseaux informatiques d'Entreprises\n",
      "#######\n",
      "b513bcf5-06b2-43ee-8c1d-51f4e79dfa1e\n",
      "None\n",
      "ROR : Relations Opérateurs de Réseaux\n",
      "#######\n",
      "1b1406e5-6383-496c-8b8e-fcc28ccbf019\n",
      "None\n",
      "ATRD : Ou \"Tarifs ATRD\" : tarifs d’Accès des Tiers aux Réseaux de Distribution\n",
      "#######\n",
      "6d27ca44-e1d9-429d-8619-21bce2831587\n",
      "None\n",
      "DPBE : Dispositif de Protection des Branchements Existants\n",
      "#######\n",
      "async activated\n",
      "========================================\n",
      "Oui, la surveillance des réseaux doit inclure la surveillance des branchements. Voici les principales actions à entreprendre :\n",
      "\n",
      "1- Vérifier l'exhaustivité de la surveillance des canalisations de la zone.\n",
      "2- Enregistrer de manière traçable les indices relevés, en notant la valeur de la concentration lue sur le détecteur, la position, ainsi que des informations complémentaires sur l'environnement.\n",
      "3- Identifier systématiquement les indices confirmés sur l'enregistrement associé au détecteur, sur le plan de réseaux transmis par l'entreprise, et sur le compte rendu de surveillance.\n",
      "4- Lorsque des rues perpendiculaires débouchent sur la partie à surveiller, les surveiller également sur une distance de 50 mètres et sur chaque trottoir, pour confirmation d'indices.\n",
      "\n",
      "La surveillance des branchements fait donc partie intégrante de la surveillance des réseaux, afin d'assurer une surveillance complète et traçable du réseau gazier.\n",
      "========================================\n",
      "Execution Time: 28.65536379814148 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95b1ef-84dd-46ab-b076-e8022df5447f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Claude Sonnet -- avec sousquestion -- 22 seconds / 4 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a65fdb22-4c9c-4a33-b9eb-dc14bf382311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.indices.utils import (\n",
    "    default_format_node_batch_fn,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.output_parsers.pydantic import PydanticOutputParser\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional, Type, Union\n",
    "\n",
    "SUB_QUESTION_PROMPT_STR = \"\"\"\n",
    "Vous êtes un assistant utile qui génère plusieurs requêtes de recherche basées sur une seule requête d'entrée. \n",
    "Générer 2 requêtes de recherche, une sur chaque ligne, liées à la requête d'entrée suivante:\n",
    "Requête: {query}\n",
    "Requêtes:\n",
    "\"\"\"\n",
    "\n",
    "PYDANTIC_FORMAT_TMPL = \"\"\"\n",
    "Voici un schéma JSON à suivre:\n",
    "{schema}\n",
    "\n",
    "Générez un objet JSON valide mais ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_SELECT_PROMPT_TMPL = \"\"\"\n",
    "Une liste de documents est présentée ci-dessous. Chaque document est accompagné d'un numéro et d'un résumé du document. Une question est également fournie.\n",
    "Donne-moi uniquement les numéros des documents pertinents à la question en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "...\n",
    "\n",
    "Document 10:\n",
    "<résumé du document 10>\n",
    "\n",
    "Question: <question>\n",
    "Tableau des nombres des documents pertinents: [2, 4, 5, 6]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "{context_str}\n",
    "Question: {query_str}\n",
    "Tableau des nombres des documents pertinents:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents pertinents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_PROMPT_TMPL = \"\"\"\n",
    "Voici une liste de contextes pertinents pour une requête donnée. Vous devez filtrer cette liste pour exclure les contextes qui correspondent à un contexte négatif spécifique.\n",
    "Donne-moi uniquement les numéros des documents en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Requête principale : <question>\n",
    "Contexte négatif : <contexte_negatif>\n",
    "Liste des contextes :\n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "Document 3:\n",
    "<résumé du document 10>\n",
    "\n",
    "Document 4:\n",
    "<résumé du document 10>\n",
    "\n",
    "Tableau des nombres des documents: [1, 2, 3]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "Requête principale : {query_str}\n",
    "Contexte négatif : {contexte_negatif}\n",
    "Liste des contextes : {contexte}\n",
    "Tableau des nombres des documents:\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "class DocumentNumberInfo(BaseModel):\n",
    "    \"\"\"Informations concernant un tableau structuré.\"\"\"\n",
    "    tableau_document: list = Field(\n",
    "        ..., description=\"le tableau des nombres des documents\"\n",
    "    )\n",
    "\n",
    "class SubQuestionEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "\n",
    "class RerankerValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerBatchEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "    batch_nodes: list\n",
    "\n",
    "class RerankerValidationDone(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerFlow(Workflow):\n",
    "    def __init__(self, vector_retriever: VectorIndexRetriever, timeout = 120, verbose = True):\n",
    "        self.llm = Bedrock(model=\"anthropic.claude-3-sonnet-20240229-v1:0\", region_name=\"eu-west-3\")\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.batch_size = 3 # the divided size to rerank all the nodes \n",
    "        self.max_retries = 3 # the max retries for the prompt\n",
    "        self.min_chunk = 10 # the minimum size of the chunks after the rerank\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "\n",
    "    # the state to generate sub questions and put inside nodes\n",
    "    @step()\n",
    "    async def sub_question(\n",
    "        self, ev: StartEvent\n",
    "    ) -> SubQuestionEvent:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        subquestions = []\n",
    "        try:\n",
    "            subquestions_response = Settings.llm.complete(SUB_QUESTION_PROMPT_STR.format(query=query))\n",
    "            subquestions = subquestions_response.text.split(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        if len(subquestions) > 2:\n",
    "            nodes1 = self.vector_retriever.retrieve(subquestions[-2])\n",
    "            nodes2 = self.vector_retriever.retrieve(subquestions[-1])\n",
    "            one_third_index = int(len(nodes)/3)\n",
    "            two_third_index = int(len(nodes)/3*2)\n",
    "            nodes_ids = []\n",
    "            for node in nodes:\n",
    "                nodes_ids.append(node.id_)\n",
    "            indicator1 = 0\n",
    "            indicator2 = 0\n",
    "            for i in range(len(nodes1)):\n",
    "                if nodes1[i].id_ not in nodes_ids:\n",
    "                    indicator1 += 1\n",
    "                    nodes_ids.append(nodes1[i].id_)\n",
    "                    if indicator1 == 1:\n",
    "                        nodes.insert(one_third_index, nodes1[i])\n",
    "                    elif indicator1 == 2:\n",
    "                        nodes.insert(one_third_index, nodes1[i])\n",
    "                    elif indicator1 == 3:\n",
    "                        nodes.insert(two_third_index, nodes1[i])\n",
    "                    elif indicator1 == 4:\n",
    "                        nodes.insert(two_third_index, nodes1[i])\n",
    "                    elif indicator1 == 5:\n",
    "                        nodes.append(nodes1[i])\n",
    "                    elif indicator1 == 6:\n",
    "                        nodes.append(nodes1[i])\n",
    "                        break\n",
    "            for i in range(len(nodes2)):\n",
    "                if nodes2[i].id_ not in nodes_ids:\n",
    "                    indicator2 += 1\n",
    "                    nodes_ids.append(nodes2[i].id_)\n",
    "                    if indicator2 == 1:\n",
    "                        nodes.insert(one_third_index, nodes2[i])\n",
    "                    elif indicator2 == 2:\n",
    "                        nodes.insert(one_third_index, nodes2[i])\n",
    "                    elif indicator2 == 3:\n",
    "                        nodes.insert(two_third_index, nodes2[i])\n",
    "                    elif indicator2 == 4:\n",
    "                        nodes.insert(two_third_index, nodes2[i])\n",
    "                    elif indicator2 == 5:\n",
    "                        nodes.append(nodes2[i])\n",
    "                    elif indicator2 == 6:\n",
    "                        nodes.append(nodes2[i])\n",
    "                        break\n",
    "        return SubQuestionEvent(query=query, neg_context=neg_context, nodes=nodes)\n",
    "\n",
    "    # the state to generate prompt for reranking\n",
    "    @step(pass_context=True)\n",
    "    async def reranker_prompt(\n",
    "        self, ctx: Context, ev: Union[SubQuestionEvent, RerankerValidationErrorEvent, RerankerBatchEvent]\n",
    "    ) -> Union[StopEvent, RerankerDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        window = int(len(nodes)/3)\n",
    "\n",
    "        if isinstance(ev, SubQuestionEvent):\n",
    "            rerank_nodes = []\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, RerankerBatchEvent):\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            # iterate the nodes based on batch size\n",
    "            if current_batch >= self.batch_size:\n",
    "                print(\"Reranked nodes are extracted.\")\n",
    "                # if no negative context, output reranked nodes. otherwise, go the state of filter\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        # prompt error event\n",
    "        elif isinstance(ev, RerankerValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            if current_retries >= self.max_retries:\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = CHOICE_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        \n",
    "        prompt = CHOICE_SELECT_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return RerankerDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes, batch_nodes=batch_nodes)\n",
    "\n",
    "    # call LLM to rerank nodes\n",
    "    @step()\n",
    "    async def rerank_validate(\n",
    "        self, ev: RerankerDone\n",
    "    ) -> Union[StopEvent, RerankerValidationDone, RerankerValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            choice_output_parser = PydanticOutputParser(output_cls=DocumentNumberInfo, pydantic_format_tmpl=PYDANTIC_FORMAT_TMPL)\n",
    "            choice_program = LLMTextCompletionProgram.from_defaults(\n",
    "                output_parser=choice_output_parser,\n",
    "                llm=self.llm,\n",
    "                prompt_template_str=ev.prompt,\n",
    "            )\n",
    "            output = choice_program(context_str=default_format_node_batch_fn(ev.batch_nodes), query_str=ev.query)\n",
    "            new_nodes = []\n",
    "            for i in output.tableau_document:\n",
    "                if i-1 < len(ev.batch_nodes):\n",
    "                    new_nodes.append(ev.batch_nodes[i-1])\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return RerankerValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        rerank_nodes = new_nodes + ev.rerank_nodes\n",
    "        # if reranked nodes are less than min chunks, return to the state of batch event, otherwise output the reranked nodes.\n",
    "        if len(rerank_nodes) > self.min_chunk:\n",
    "            print(\"Reranked nodes are extracted.\")\n",
    "            if ev.neg_context == '':\n",
    "                return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                return RerankerValidationDone(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "        else:\n",
    "            return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # prepare the prompt to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_prompt(\n",
    "        self, ctx: Context, ev: Union[RerankerValidationDone, FilterValidationErrorEvent]\n",
    "    ) -> Union[StopEvent, FilterDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        rerank_nodes = ev.rerank_nodes\n",
    "\n",
    "        if isinstance(ev, RerankerValidationDone):\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, FilterValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            if current_retries >= self.max_retries:\n",
    "                if len(rerank_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            reflection_prompt = NEGATIVE_FILTER_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        prompt = NEGATIVE_FILTER_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return FilterDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # call LLM to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_validate(\n",
    "        self, ctx: Context, ev: FilterDone\n",
    "    ) -> Union[StopEvent, FilterValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            choice_output_parser = PydanticOutputParser(output_cls=DocumentNumberInfo, pydantic_format_tmpl=PYDANTIC_FORMAT_TMPL)\n",
    "            filter_program = LLMTextCompletionProgram.from_defaults(\n",
    "                output_parser=choice_output_parser,\n",
    "                llm=self.llm,\n",
    "                prompt_template_str=ev.prompt,\n",
    "            )\n",
    "            output = filter_program(context_str=default_format_node_batch_fn(ev.rerank_nodes), query_str=ev.query, contexte_negatif=ev.neg_context)\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return FilterValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        new_nodes = []\n",
    "        for i in output.tableau_document:\n",
    "            new_nodes.append(ev.rerank_nodes[i-1])\n",
    "        print(\"Reranked nodes are filtered.\")\n",
    "        current_batch = ctx.data.get(\"batch\", 0)\n",
    "        if len(new_nodes) > self.min_chunk:\n",
    "            return StopEvent(result=new_nodes)\n",
    "        else:\n",
    "            if current_batch >= self.batch_size:\n",
    "                if len(new_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=new_nodes)\n",
    "            else:\n",
    "                return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c79cc95-4956-4acd-914a-df266f02bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.indices.keyword_table import KeywordTableGPTRetriever\n",
    "from typing import List\n",
    "import threading\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableGPTRetriever,\n",
    "        embed_model: BedrockEmbedding,\n",
    "        old_queries: List = [],\n",
    "        mode: str = \"OR\",\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self.embed_model = embed_model\n",
    "        self.reranker = RerankerFlow(vector_retriever=vector_retriever)\n",
    "        self.old_queries = old_queries\n",
    "        index_feedback = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"finetune/vector_persist_800_feedback\"))\n",
    "        self.feedback_retriever = VectorIndexRetriever(index=index_feedback, similarity_top_k=1)\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async not activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = self._weighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = await self._aweighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        feedback_nodes = self.feedback_retriever.retrieve(query_bundle)\n",
    "        if feedback_nodes[0].score > 0.8 : \n",
    "            neg_context = feedback_nodes[0].metadata[\"message\"]\n",
    "        else:\n",
    "            neg_context = ''\n",
    "        vector_nodes = await self.reranker.run(query=query_bundle.query_str, nodes=vector_nodes_full, neg_context=neg_context)\n",
    "        if vector_nodes == None:\n",
    "            return keyword_nodes\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    def replace_abbreviations(self, question, keyword_nodes):\n",
    "        \"\"\"\n",
    "        Replaces abbreviations in the lexique with their complete names.\n",
    "        Assumes abbreviations are separated by either two spaces or one space + one punctuation mark.\n",
    "        \"\"\"\n",
    "        lexique = {}\n",
    "        for node in keyword_nodes:\n",
    "            keyword = node.node.metadata['keyword']\n",
    "            keyword_complete = node.text.split(': ')[1]\n",
    "            lexique[keyword] = keyword_complete\n",
    "\n",
    "        words = question.split()\n",
    "        result = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in lexique:\n",
    "                result.append(lexique[word])\n",
    "                result.append('('+word+')')\n",
    "            elif word[:-1] in lexique:\n",
    "                result.append(lexique[word[:-1]])\n",
    "                result.append('('+word[:-1]+')')\n",
    "                result.append(word[-1])\n",
    "            elif word[1:] in lexique:\n",
    "                result.append(word[0])\n",
    "                result.append(lexique[word[1:]])\n",
    "                result.append('('+word[1:]+')')\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def _get_query_embedding_threaded(self, query):\n",
    "        \"\"\"\n",
    "        Get the query embedding using a separate thread.\n",
    "        \"\"\"\n",
    "        result_queue = Queue()  # Create a queue to store the result\n",
    "        thread = threading.Thread(target=self._get_query_embedding_async_thread, args=(query, result_queue))\n",
    "        thread.start()\n",
    "        thread.join()  # Wait for the thread to finish\n",
    "        return result_queue.get()  # Retrieve the result from the queue\n",
    "\n",
    "    def _get_query_embedding_async_thread(self, query, result_queue):\n",
    "        \"\"\"\n",
    "        Helper method to run the asynchronous call in a separate thread.\n",
    "        \"\"\"\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(self.embed_model.aget_query_embedding(query))\n",
    "        loop.close()\n",
    "        result_queue.put(result)  # Put the result in the queue\n",
    "    \n",
    "    def _weighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = self._get_query_embedding_threaded(new_query_str)\n",
    "        print(current_embedding)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(self._get_query_embedding_threaded(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding\n",
    "    async def _aweighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = await self.embed_model.aget_query_embedding(new_query_str)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(await self.embed_model.aget_query_embedding(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95815e19-300c-47a8-9faa-79a2dbc62937",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index_doc, similarity_top_k=30)\n",
    "keyword_retriever = KeywordTableGPTRetriever(index=index_keyword, max_keywords_per_query = 20, keyword_extract_template=KEYWORD_EXTRACT_TEMPLATE)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever, embed_model=Settings.embed_model, old_queries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc354c8e-db3a-41ed-9a0d-a3f539f1deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "qa_template = PromptTemplate(DEFAULT_QUERY_PROMPT_TMPL)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=custom_retriever,\n",
    "                                              text_qa_template=qa_template,\n",
    "                                              use_async=True,\n",
    "                                              response_mode = ResponseMode.SIMPLE_SUMMARIZE,\n",
    "                                              streaming=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "032166fa-45d4-4a41-83a2-a5c0d955bd86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Reranked nodes are filtered.\n",
      "Step filter_validate produced event StopEvent\n",
      "========================================\n",
      "Selon les informations fournies, pour faire une liaison détente/comptage pour alimenter une chaufferie, il faut utiliser des tubes en acier conformes aux normes suivantes :\n",
      "\n",
      "1- Les tubes noirs doivent avoir un rapport de diamètre de piquage sur diamètre du tube principal inférieur ou égal à 1.\n",
      "2- Les tubes galvanisés doivent avoir un rapport de diamètre de piquage sur diamètre du tube principal inférieur ou égal à 2/3.\n",
      "3- Les piquages directs sont interdits pour les tubes conformes à l'ancienne norme NF A 49-146.\n",
      "4- En cas d'utilisation de raccords mécaniques, ceux-ci doivent être conformes aux normes spécifiées au paragraphe 4.3.2.2.\n",
      "========================================\n",
      "Execution Time: 26.454052448272705 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96b9b301-113c-492f-8309-746868b72c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "========================================\n",
      "Les informations contextuelles ne mentionnent pas d'achats immobilisés spécifiques au biométhane. Cependant, elles indiquent que le poste réseau d'injection de biométhane comprend différents équipements tels que :\n",
      "\n",
      "1- Des organes de coupure accessibles depuis le domaine public.\n",
      "2- Un limiteur de débit dynamique pour moduler l'injection en fonction de la pression amont.\n",
      "3- Des capteurs de pression et de débit pour l'instrumentation du réseau.\n",
      "4- Des équipements pour le contrôle de la qualité du gaz, son odorisation, sa pression et la régulation de son débit.\n",
      "\n",
      "Ces équipements font partie des achats immobilisés nécessaires à l'injection de biométhane dans le réseau de distribution de gaz naturel.\n",
      "========================================\n",
      "Execution Time: 19.82956886291504 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quels sont les achats immobilisés pour le biométhane ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ebe54721-214e-482c-b9f5-d89f90fb431d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "========================================\n",
      "CICM signifie \"Conduite d'Immeuble, Conduite Montante\". C'est un terme utilisé pour désigner les tuyauteries verticales qui alimentent différents niveaux d'un bâtiment d'habitation collectif, ainsi que les conduites reliant ces tuyauteries verticales à la conduite d'immeuble.\n",
      "========================================\n",
      "Execution Time: 18.33278775215149 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Que veut dire CICM ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08787d72-3787-47d2-9065-19d089d5e9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Reranked nodes are filtered.\n",
      "Step filter_validate produced event StopEvent\n",
      "========================================\n",
      "Oui, la surveillance des réseaux doit inclure le suivi des branchements. Voici les principales actions à entreprendre :\n",
      "\n",
      "1- Effectuer un contrôle de cohérence entre les données figurant sur les plans et les éléments constatés sur le terrain, en notant les écarts éventuellement observés.\n",
      "\n",
      "2- Mettre en œuvre des méthodes de détection électromagnétique, acoustique ou par géoradar, en fonction des caractéristiques des branchements (matière, implantation, type de robinet) et des limites de chaque technique.\n",
      "\n",
      "3- Privilégier l'utilisation du Flexitrace pour les branchements en polyéthylène moyenne pression basse improductifs ou avec un compteur inactif et un coffret en élévation, en raison de la fiabilité de cette méthode et de l'absence de gêne pour le client.\n",
      "\n",
      "4- Prioriser le traitement des zones comportant des réseaux installés entre 2000 et 2008, conformément à la réglementation en vigueur.\n",
      "\n",
      "5- Étendre, si possible, le périmètre de traitement à la rue entière ou à un ensemble de rues, en fonction de la densité des branchements, afin d'optimiser les interventions.\n",
      "========================================\n",
      "Execution Time: 23.684138536453247 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b28022-7310-4ff8-b39b-7e91bec3c85b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Claude Sonnet -- sans sousquestion  -- 15.9 seconds / 4 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f068cb-5c36-4f93-a887-9f870724138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.indices.utils import (\n",
    "    default_format_node_batch_fn,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.output_parsers.pydantic import PydanticOutputParser\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional, Type, Union\n",
    "\n",
    "SUB_QUESTION_PROMPT_STR = \"\"\"\n",
    "Vous êtes un assistant utile qui génère plusieurs requêtes de recherche basées sur une seule requête d'entrée. \n",
    "Générer 2 requêtes de recherche, une sur chaque ligne, liées à la requête d'entrée suivante:\n",
    "Requête: {query}\n",
    "Requêtes:\n",
    "\"\"\"\n",
    "\n",
    "PYDANTIC_FORMAT_TMPL = \"\"\"\n",
    "Voici un schéma JSON à suivre:\n",
    "{schema}\n",
    "\n",
    "Générez un objet JSON valide mais ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_SELECT_PROMPT_TMPL = \"\"\"\n",
    "Une liste de documents est présentée ci-dessous. Chaque document est accompagné d'un numéro et d'un résumé du document. Une question est également fournie.\n",
    "Donne-moi uniquement les numéros des documents pertinents à la question en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "...\n",
    "\n",
    "Document 10:\n",
    "<résumé du document 10>\n",
    "\n",
    "Question: <question>\n",
    "Tableau des nombres des documents pertinents: [2, 4, 5, 6]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "{context_str}\n",
    "Question: {query_str}\n",
    "Tableau des nombres des documents pertinents:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents pertinents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_PROMPT_TMPL = \"\"\"\n",
    "Voici une liste de contextes pertinents pour une requête donnée. Vous devez filtrer cette liste pour exclure les contextes qui correspondent à un contexte négatif spécifique.\n",
    "Donne-moi uniquement les numéros des documents en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Requête principale : <question>\n",
    "Contexte négatif : <contexte_negatif>\n",
    "Liste des contextes :\n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "Document 3:\n",
    "<résumé du document 10>\n",
    "\n",
    "Document 4:\n",
    "<résumé du document 10>\n",
    "\n",
    "Tableau des nombres des documents: [1, 2, 3]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "Requête principale : {query_str}\n",
    "Contexte négatif : {contexte_negatif}\n",
    "Liste des contextes : {contexte}\n",
    "Tableau des nombres des documents:\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "class DocumentNumberInfo(BaseModel):\n",
    "    \"\"\"Informations concernant un tableau structuré.\"\"\"\n",
    "    tableau_document: list = Field(\n",
    "        ..., description=\"le tableau des nombres des documents\"\n",
    "    )\n",
    "\n",
    "class SubQuestionEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "\n",
    "class RerankerValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerBatchEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "    batch_nodes: list\n",
    "\n",
    "class RerankerValidationDone(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerFlow(Workflow):\n",
    "    def __init__(self, vector_retriever: VectorIndexRetriever, timeout = 120, verbose = True):\n",
    "        self.llm = Bedrock(model=\"anthropic.claude-3-sonnet-20240229-v1:0\", region_name=\"eu-west-3\")\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.batch_size = 3 # the divided size to rerank all the nodes \n",
    "        self.max_retries = 3 # the max retries for the prompt\n",
    "        self.min_chunk = 10 # the minimum size of the chunks after the rerank\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "\n",
    "    # the state to generate sub questions and put inside nodes\n",
    "    @step()\n",
    "    async def sub_question(\n",
    "        self, ev: StartEvent\n",
    "    ) -> SubQuestionEvent:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        # subquestions = []\n",
    "        # try:\n",
    "        #     subquestions_response = Settings.llm.complete(SUB_QUESTION_PROMPT_STR.format(query=query))\n",
    "        #     subquestions = subquestions_response.text.split(\"\\n\")\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        # if len(subquestions) > 2:\n",
    "        #     nodes1 = self.vector_retriever.retrieve(subquestions[-2])\n",
    "        #     nodes2 = self.vector_retriever.retrieve(subquestions[-1])\n",
    "        #     one_third_index = int(len(nodes)/3)\n",
    "        #     two_third_index = int(len(nodes)/3*2)\n",
    "        #     nodes_ids = []\n",
    "        #     for node in nodes:\n",
    "        #         nodes_ids.append(node.id_)\n",
    "        #     indicator1 = 0\n",
    "        #     indicator2 = 0\n",
    "        #     for i in range(len(nodes1)):\n",
    "        #         if nodes1[i].id_ not in nodes_ids:\n",
    "        #             indicator1 += 1\n",
    "        #             nodes_ids.append(nodes1[i].id_)\n",
    "        #             if indicator1 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 5:\n",
    "        #                 nodes.append(nodes1[i])\n",
    "        #             elif indicator1 == 6:\n",
    "        #                 nodes.append(nodes1[i])\n",
    "        #                 break\n",
    "        #     for i in range(len(nodes2)):\n",
    "        #         if nodes2[i].id_ not in nodes_ids:\n",
    "        #             indicator2 += 1\n",
    "        #             nodes_ids.append(nodes2[i].id_)\n",
    "        #             if indicator2 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 5:\n",
    "        #                 nodes.append(nodes2[i])\n",
    "        #             elif indicator2 == 6:\n",
    "        #                 nodes.append(nodes2[i])\n",
    "        #                 break\n",
    "        return SubQuestionEvent(query=query, neg_context=neg_context, nodes=nodes)\n",
    "\n",
    "    # the state to generate prompt for reranking\n",
    "    @step(pass_context=True)\n",
    "    async def reranker_prompt(\n",
    "        self, ctx: Context, ev: Union[SubQuestionEvent, RerankerValidationErrorEvent, RerankerBatchEvent]\n",
    "    ) -> Union[StopEvent, RerankerDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        window = int(len(nodes)/3)\n",
    "\n",
    "        if isinstance(ev, SubQuestionEvent):\n",
    "            rerank_nodes = []\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, RerankerBatchEvent):\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            # iterate the nodes based on batch size\n",
    "            if current_batch >= self.batch_size:\n",
    "                print(\"Reranked nodes are extracted.\")\n",
    "                # if no negative context, output reranked nodes. otherwise, go the state of filter\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        # prompt error event\n",
    "        elif isinstance(ev, RerankerValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            if current_retries >= self.max_retries:\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = CHOICE_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        \n",
    "        prompt = CHOICE_SELECT_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return RerankerDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes, batch_nodes=batch_nodes)\n",
    "\n",
    "    # call LLM to rerank nodes\n",
    "    @step()\n",
    "    async def rerank_validate(\n",
    "        self, ev: RerankerDone\n",
    "    ) -> Union[StopEvent, RerankerValidationDone, RerankerValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            choice_output_parser = PydanticOutputParser(output_cls=DocumentNumberInfo, pydantic_format_tmpl=PYDANTIC_FORMAT_TMPL)\n",
    "            choice_program = LLMTextCompletionProgram.from_defaults(\n",
    "                output_parser=choice_output_parser,\n",
    "                llm=self.llm,\n",
    "                prompt_template_str=ev.prompt,\n",
    "            )\n",
    "            output = choice_program(context_str=default_format_node_batch_fn(ev.batch_nodes), query_str=ev.query)\n",
    "            new_nodes = []\n",
    "            for i in output.tableau_document:\n",
    "                if i-1 < len(ev.batch_nodes):\n",
    "                    new_nodes.append(ev.batch_nodes[i-1])\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return RerankerValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        rerank_nodes = new_nodes + ev.rerank_nodes\n",
    "        # if reranked nodes are less than min chunks, return to the state of batch event, otherwise output the reranked nodes.\n",
    "        if len(rerank_nodes) > self.min_chunk:\n",
    "            print(\"Reranked nodes are extracted.\")\n",
    "            if ev.neg_context == '':\n",
    "                return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                return RerankerValidationDone(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "        else:\n",
    "            return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # prepare the prompt to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_prompt(\n",
    "        self, ctx: Context, ev: Union[RerankerValidationDone, FilterValidationErrorEvent]\n",
    "    ) -> Union[StopEvent, FilterDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        rerank_nodes = ev.rerank_nodes\n",
    "\n",
    "        if isinstance(ev, RerankerValidationDone):\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, FilterValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            if current_retries >= self.max_retries:\n",
    "                if len(rerank_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            reflection_prompt = NEGATIVE_FILTER_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        prompt = NEGATIVE_FILTER_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return FilterDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # call LLM to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_validate(\n",
    "        self, ctx: Context, ev: FilterDone\n",
    "    ) -> Union[StopEvent, FilterValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            choice_output_parser = PydanticOutputParser(output_cls=DocumentNumberInfo, pydantic_format_tmpl=PYDANTIC_FORMAT_TMPL)\n",
    "            filter_program = LLMTextCompletionProgram.from_defaults(\n",
    "                output_parser=choice_output_parser,\n",
    "                llm=self.llm,\n",
    "                prompt_template_str=ev.prompt,\n",
    "            )\n",
    "            output = filter_program(context_str=default_format_node_batch_fn(ev.rerank_nodes), query_str=ev.query, contexte_negatif=ev.neg_context)\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return FilterValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        new_nodes = []\n",
    "        for i in output.tableau_document:\n",
    "            new_nodes.append(ev.rerank_nodes[i-1])\n",
    "        print(\"Reranked nodes are filtered.\")\n",
    "        current_batch = ctx.data.get(\"batch\", 0)\n",
    "        if len(new_nodes) > self.min_chunk:\n",
    "            return StopEvent(result=new_nodes)\n",
    "        else:\n",
    "            if current_batch >= self.batch_size:\n",
    "                if len(new_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=new_nodes)\n",
    "            else:\n",
    "                return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c02212-111f-4c0f-be10-101878e6fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.indices.keyword_table import KeywordTableGPTRetriever\n",
    "from typing import List\n",
    "import threading\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableGPTRetriever,\n",
    "        embed_model: BedrockEmbedding,\n",
    "        old_queries: List = [],\n",
    "        mode: str = \"OR\",\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self.embed_model = embed_model\n",
    "        self.reranker = RerankerFlow(vector_retriever=vector_retriever)\n",
    "        self.old_queries = old_queries\n",
    "        index_feedback = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"finetune/vector_persist_800_feedback\"))\n",
    "        self.feedback_retriever = VectorIndexRetriever(index=index_feedback, similarity_top_k=1)\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async not activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = self._weighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = await self._aweighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        feedback_nodes = self.feedback_retriever.retrieve(query_bundle)\n",
    "        if feedback_nodes[0].score > 0.8 : \n",
    "            neg_context = feedback_nodes[0].metadata[\"message\"]\n",
    "        else:\n",
    "            neg_context = ''\n",
    "        vector_nodes = await self.reranker.run(query=query_bundle.query_str, nodes=vector_nodes_full, neg_context=neg_context)\n",
    "        if vector_nodes == None:\n",
    "            return keyword_nodes\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    def replace_abbreviations(self, question, keyword_nodes):\n",
    "        \"\"\"\n",
    "        Replaces abbreviations in the lexique with their complete names.\n",
    "        Assumes abbreviations are separated by either two spaces or one space + one punctuation mark.\n",
    "        \"\"\"\n",
    "        lexique = {}\n",
    "        for node in keyword_nodes:\n",
    "            keyword = node.node.metadata['keyword']\n",
    "            keyword_complete = node.text.split(': ')[1]\n",
    "            lexique[keyword] = keyword_complete\n",
    "\n",
    "        words = question.split()\n",
    "        result = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in lexique:\n",
    "                result.append(lexique[word])\n",
    "                result.append('('+word+')')\n",
    "            elif word[:-1] in lexique:\n",
    "                result.append(lexique[word[:-1]])\n",
    "                result.append('('+word[:-1]+')')\n",
    "                result.append(word[-1])\n",
    "            elif word[1:] in lexique:\n",
    "                result.append(word[0])\n",
    "                result.append(lexique[word[1:]])\n",
    "                result.append('('+word[1:]+')')\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def _get_query_embedding_threaded(self, query):\n",
    "        \"\"\"\n",
    "        Get the query embedding using a separate thread.\n",
    "        \"\"\"\n",
    "        result_queue = Queue()  # Create a queue to store the result\n",
    "        thread = threading.Thread(target=self._get_query_embedding_async_thread, args=(query, result_queue))\n",
    "        thread.start()\n",
    "        thread.join()  # Wait for the thread to finish\n",
    "        return result_queue.get()  # Retrieve the result from the queue\n",
    "\n",
    "    def _get_query_embedding_async_thread(self, query, result_queue):\n",
    "        \"\"\"\n",
    "        Helper method to run the asynchronous call in a separate thread.\n",
    "        \"\"\"\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(self.embed_model.aget_query_embedding(query))\n",
    "        loop.close()\n",
    "        result_queue.put(result)  # Put the result in the queue\n",
    "    \n",
    "    def _weighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = self._get_query_embedding_threaded(new_query_str)\n",
    "        print(current_embedding)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(self._get_query_embedding_threaded(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding\n",
    "    async def _aweighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = await self.embed_model.aget_query_embedding(new_query_str)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(await self.embed_model.aget_query_embedding(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f914248-e274-49b8-9257-a8bd9898ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "qa_template = PromptTemplate(DEFAULT_QUERY_PROMPT_TMPL)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=custom_retriever,\n",
    "                                              text_qa_template=qa_template,\n",
    "                                              use_async=True,\n",
    "                                              response_mode = ResponseMode.SIMPLE_SUMMARIZE,\n",
    "                                              streaming=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2c6ddcd4-deac-406e-b0a4-44bcd7c2c92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Reranked nodes are filtered.\n",
      "Step filter_validate produced event StopEvent\n",
      "========================================\n",
      "Selon les informations fournies, pour réaliser une liaison détente/comptage pour alimenter une chaufferie, les normes de tubes acier à utiliser sont les suivantes :\n",
      "\n",
      "1- Les tubes acier doivent être conformes aux normes NF EN 10216-1, NF A 49-115, NF A 49-141, NF EN 10217-1 et NF A 49-145.\n",
      "2- L'exécution des piquages sur ces tubes est autorisée, à condition que le rapport entre le diamètre du piquage et le diamètre du tube soit :\n",
      "- Inférieur ou égal à 1 pour les tubes noirs,\n",
      "- Inférieur ou égal à 2/3 pour les tubes galvanisés.\n",
      "3- Pour les compléments d'installation réalisés en tubes conformes à l'ancienne norme NF A 49-146, les piquages directs sont interdits.\n",
      "4- En cas d'utilisation de raccords mécaniques, ceux-ci doivent être conformes aux normes spécifiées au paragraphe 4.3.2.2.\n",
      "Execution Time: 25.057605504989624 seconds\n"
     ]
    }
   ],
   "source": [
    "# with sub_question\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd93672-592d-48b4-9111-fc612e78e29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Reranked nodes are extracted.\n",
      "Step reranker_prompt produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Reranked nodes are filtered.\n",
      "Step filter_validate produced event StopEvent\n",
      "========================================\n",
      "Selon les informations fournies, pour les tubes en acier, les raccords doivent être conformes aux spécifications ATG B 521. De plus, le tableau 1 indique que les tubes des normes NF EN 10216-1, NF A 49-115, NF A 49-141, NF EN 10217-1, NF A 49-145, noirs ou galvanisés, sont autorisés pour le soudage électrique sous atmosphère neutre, le soudage oxyacétylénique (si l'épaisseur du tube est inférieure ou égale à 3,2 mm) et le soudo-brasage. Par conséquent, pour faire une liaison détente/comptage pour alimenter une chaufferie, il est recommandé d'utiliser des tubes en acier conformes à ces normes.\n",
      "========================================\n",
      "Execution Time: 17.7623553276062 seconds\n"
     ]
    }
   ],
   "source": [
    "# without sub_question\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "10ad0bbb-0a9a-4798-81c0-5dd20d14a93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "========================================\n",
      "Il n'y a pas d'informations spécifiques sur les achats immobilisés pour le biométhane dans le contexte fourni. Les documents traitent principalement des aspects techniques et réglementaires liés à l'injection de biométhane dans le réseau de distribution de gaz naturel. Ils abordent des sujets tels que :\n",
      "\n",
      "- Les caractéristiques techniques des postes d'injection de biométhane, notamment les systèmes d'odorisation, d'analyse du gaz et de régulation du débit.\n",
      "- Les procédures de raccordement des installations de production de biométhane au réseau de distribution.\n",
      "- Les contrats d'injection de biométhane entre le producteur et le concessionnaire du réseau.\n",
      "- Les aspects de maintenance et de suivi des postes d'injection de biométhane.\n",
      "- Les priorités d'injection du biométhane dans le réseau de distribution.\n",
      "\n",
      "Le contexte ne fournit pas d'informations sur les achats immobilisés liés au biométhane.\n",
      "========================================\n",
      "Execution Time: 16.450987339019775 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quels sont les achats immobilisés pour le biométhane ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9b55cb6b-011e-4785-942d-b294d3f854c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "========================================\n",
      "CICM signifie \"Conduite d'Immeuble, Conduite Montante\". C'est un terme utilisé pour désigner les tuyauteries verticales qui alimentent différents niveaux d'un bâtiment d'habitation collectif, ainsi que les conduites d'immeuble qui raccordent ces tuyauteries verticales à la conduite principale.\n",
      "========================================\n",
      "Execution Time: 12.03987431526184 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Que veut dire CICM ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "46ef8e50-a284-4d67-b942-29cee81d515b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Reranked nodes are filtered.\n",
      "Step filter_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Reranked nodes are extracted.\n",
      "Step reranker_prompt produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Reranked nodes are filtered.\n",
      "Step filter_validate produced event StopEvent\n",
      "========================================\n",
      "Oui, la surveillance des réseaux de gaz naturel doit inclure la surveillance des branchements. D'après les informations fournies :\n",
      "\n",
      "- Le Véhicule de Surveillance des Réseaux (VSR) permet la surveillance des canalisations de réseaux ainsi que des branchements situés dans son périmètre de détection.\n",
      "- En complément de la surveillance régulière, une surveillance renforcée peut être nécessaire pour certaines zones, notamment les réseaux soumis à des risques particuliers comme les affaissements miniers ou les glissements de terrain.\n",
      "- Avant les opérations de surveillance, un contrôle de cohérence entre les données cartographiques et les éléments constatés sur le terrain est effectué, et les écarts éventuels sont notés.\n",
      "- Différentes méthodes de détection, comme l'électromagnétique, l'acoustique ou le géoradar, sont utilisées en fonction des caractéristiques des branchements (matériau, implantation, type de robinet, etc.).\n",
      "- Un accent particulier est mis sur la détection des branchements non représentés sur la cartographie, afin de qualifier le réseau en classe A.\n",
      "========================================\n",
      "Execution Time: 17.532476902008057 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf7de0-d8c7-4e0e-bafb-7a0e2dae8872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad376df-33a1-482f-be59-9fe5be25ee12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aba07a8-3f07-4234-b027-6d7e92db7c24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Claude haiku -- avec sousquestion  -- 22.3 seconds / 4 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7eebc29-f10f-44db-b2eb-255066fa6e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les informations fournies, les documents pertinents à la question \"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\" sont :\n",
      "\n",
      "[6, 7]\n",
      "\n",
      "Le document 6 traite du branchement particulier et indique que chaque branchement particulier doit être muni d'un organe de coupure individuelle (OCI) qui doit être accessible, bien signalé, facilement manœuvrable et identifié. Cela suggère que la surveillance des branchements fait partie de la surveillance des réseaux.\n",
      "\n",
      "Le document 7 décrit la conduite d'immeuble, qui fait suite au branchement d'immeuble collectif. Cela indique également que les branchements font partie intégrante du réseau à surveiller.\n",
      "======================\n",
      "D'après les résumés des documents fournis, les documents pertinents pour répondre à la question \"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\" sont les suivants :\n",
      "\n",
      "[6, 17]\n",
      "\n",
      "Le document 6 mentionne que \"Le branchement particulier est la partie d'ouvrage située immédiatement en amont du compteur et raccordant ce dernier aux parties de l'installation, commune à la desserte de plusieurs logements\". Cela suggère que les branchements font partie des réseaux à surveiller.\n",
      "\n",
      "Le document 17 fournit des détails sur les caractéristiques et l'identification des branchements particuliers, renforçant l'idée qu'ils doivent être surveillés dans le cadre de la surveillance des réseaux.\n"
     ]
    }
   ],
   "source": [
    "llm_sonnet = Bedrock(model=\"anthropic.claude-3-sonnet-20240229-v1:0\", region_name=\"eu-west-3\")\n",
    "Response_haiku = Settings.llm.complete(CHOICE_SELECT_PROMPT_TMPL.format(context_str=default_format_node_batch_fn(nodes), query_str=\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\"))\n",
    "print(Response_haiku.text)\n",
    "print(\"======================\")\n",
    "Response_sonnet = llm_sonnet.complete(CHOICE_SELECT_PROMPT_TMPL.format(context_str=default_format_node_batch_fn(nodes), query_str=\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\"))\n",
    "print(Response_sonnet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f907ebc5-bb23-48a3-b48f-feee562522e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    ")\n",
    "import re\n",
    "from llama_index.core.indices.utils import (\n",
    "    default_format_node_batch_fn,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.output_parsers.pydantic import PydanticOutputParser\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional, Type, Union\n",
    "\n",
    "SUB_QUESTION_PROMPT_STR = \"\"\"\n",
    "Vous êtes un assistant utile qui génère plusieurs requêtes de recherche basées sur une seule requête d'entrée. \n",
    "Générer 3 requêtes de recherche, une sur chaque ligne, liées à la requête d'entrée suivante:\n",
    "Requête: {query}\n",
    "Requêtes:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_SELECT_PROMPT_TMPL = \"\"\"\n",
    "Une liste de documents est présentée ci-dessous. Chaque document est accompagné d'un numéro et d'un résumé du document. Une question est également fournie.\n",
    "Donne-moi uniquement les numéros des documents pertinents à la question en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "...\n",
    "\n",
    "Document 10:\n",
    "<résumé du document 10>\n",
    "\n",
    "Question: <question>\n",
    "Tableau des nombres des documents pertinents: [2, 4, 5, 6]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "{context_str}\n",
    "Question: {query_str}\n",
    "Tableau des nombres des documents pertinents:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents pertinents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_PROMPT_TMPL = \"\"\"\n",
    "Voici une liste de contextes pertinents pour une requête donnée. Vous devez filtrer cette liste pour exclure les contextes qui correspondent à un contexte négatif spécifique.\n",
    "Donne-moi uniquement les numéros des documents en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Requête principale : <question>\n",
    "Contexte négatif : <contexte_negatif>\n",
    "Liste des contextes :\n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "Document 3:\n",
    "<résumé du document 10>\n",
    "\n",
    "Document 4:\n",
    "<résumé du document 10>\n",
    "\n",
    "Tableau des nombres des documents: [1, 2, 3]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "Requête principale : {query_str}\n",
    "Contexte négatif : {contexte_negatif}\n",
    "Liste des contextes : {contexte}\n",
    "Tableau des nombres des documents:\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "class SubQuestionEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "\n",
    "class RerankerValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerBatchEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "    batch_nodes: list\n",
    "\n",
    "class RerankerValidationDone(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerFlow(Workflow):\n",
    "    def __init__(self, vector_retriever: VectorIndexRetriever, timeout = 120, verbose = True):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.batch_size = 3 # the divided size to rerank all the nodes \n",
    "        self.max_retries = 3 # the max retries for the prompt\n",
    "        self.min_chunk = 10 # the minimum size of the chunks after the rerank\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "\n",
    "    # the state to generate sub questions and put inside nodes\n",
    "    @step()\n",
    "    async def sub_question(\n",
    "        self, ev: StartEvent\n",
    "    ) -> SubQuestionEvent:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        subquestions = []\n",
    "        try:\n",
    "            subquestions_response = Settings.llm.complete(SUB_QUESTION_PROMPT_STR.format(query=query))\n",
    "            subquestions = subquestions_response.text.split(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        if len(subquestions) > 3:\n",
    "            nodes1 = self.vector_retriever.retrieve(subquestions[-3])\n",
    "            nodes2 = self.vector_retriever.retrieve(subquestions[-2])\n",
    "            nodes3 = self.vector_retriever.retrieve(subquestions[-1])\n",
    "            one_third_index = int(len(nodes)/3)\n",
    "            two_third_index = int(len(nodes)/3*2)\n",
    "            nodes_ids = []\n",
    "            for node in nodes:\n",
    "                nodes_ids.append(node.id_)\n",
    "            indicator1 = 0\n",
    "            indicator2 = 0\n",
    "            indicator3 = 0\n",
    "            for i in range(len(nodes1)):\n",
    "                if nodes1[i].id_ not in nodes_ids:\n",
    "                    indicator1 += 1\n",
    "                    nodes_ids.append(nodes1[i].id_)\n",
    "                    if indicator1 == 1:\n",
    "                        nodes.insert(one_third_index, nodes1[i])\n",
    "                    elif indicator1 == 2:\n",
    "                        nodes.insert(one_third_index, nodes1[i])\n",
    "                    elif indicator1 == 3:\n",
    "                        nodes.insert(two_third_index, nodes1[i])\n",
    "                    elif indicator1 == 4:\n",
    "                        nodes.insert(two_third_index, nodes1[i])\n",
    "                    elif indicator1 == 5:\n",
    "                        nodes.append(nodes1[i])\n",
    "                    elif indicator1 == 6:\n",
    "                        nodes.append(nodes1[i])\n",
    "                        break\n",
    "            for i in range(len(nodes2)):\n",
    "                if nodes2[i].id_ not in nodes_ids:\n",
    "                    indicator2 += 1\n",
    "                    nodes_ids.append(nodes2[i].id_)\n",
    "                    if indicator2 == 1:\n",
    "                        nodes.insert(one_third_index, nodes2[i])\n",
    "                    elif indicator2 == 2:\n",
    "                        nodes.insert(one_third_index, nodes2[i])\n",
    "                    elif indicator2 == 3:\n",
    "                        nodes.insert(two_third_index, nodes2[i])\n",
    "                    elif indicator2 == 4:\n",
    "                        nodes.insert(two_third_index, nodes2[i])\n",
    "                    elif indicator2 == 5:\n",
    "                        nodes.append(nodes2[i])\n",
    "                    elif indicator2 == 6:\n",
    "                        nodes.append(nodes2[i])\n",
    "                        break\n",
    "            for i in range(len(nodes3)):\n",
    "                if nodes3[i].id_ not in nodes_ids:\n",
    "                    indicator3 += 1\n",
    "                    nodes_ids.append(nodes3[i].id_)\n",
    "                    if indicator3 == 1:\n",
    "                        nodes.insert(one_third_index, nodes3[i])\n",
    "                    elif indicator3 == 2:\n",
    "                        nodes.insert(one_third_index, nodes3[i])\n",
    "                    elif indicator3 == 3:\n",
    "                        nodes.insert(two_third_index, nodes3[i])\n",
    "                    elif indicator3 == 4:\n",
    "                        nodes.insert(two_third_index, nodes3[i])\n",
    "                    elif indicator3 == 5:\n",
    "                        nodes.append(nodes3[i])\n",
    "                    elif indicator3 == 6:\n",
    "                        nodes.append(nodes3[i])\n",
    "                        break\n",
    "        return SubQuestionEvent(query=query, neg_context=neg_context, nodes=nodes)\n",
    "\n",
    "    # the state to generate prompt for reranking\n",
    "    @step(pass_context=True)\n",
    "    async def reranker_prompt(\n",
    "        self, ctx: Context, ev: Union[SubQuestionEvent, RerankerValidationErrorEvent, RerankerBatchEvent]\n",
    "    ) -> Union[StopEvent, RerankerDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        window = int(len(nodes)/3)\n",
    "\n",
    "        if isinstance(ev, SubQuestionEvent):\n",
    "            rerank_nodes = []\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, RerankerBatchEvent):\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            # iterate the nodes based on batch size\n",
    "            if current_batch >= self.batch_size:\n",
    "                print(\"Reranked nodes are extracted.\")\n",
    "                # if no negative context, output reranked nodes. otherwise, go the state of filter\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        # prompt error event\n",
    "        elif isinstance(ev, RerankerValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            if current_retries >= self.max_retries:\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = CHOICE_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        \n",
    "        prompt = CHOICE_SELECT_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return RerankerDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes, batch_nodes=batch_nodes)\n",
    "\n",
    "    # call LLM to rerank nodes\n",
    "    @step()\n",
    "    async def rerank_validate(\n",
    "        self, ev: RerankerDone\n",
    "    ) -> Union[StopEvent, RerankerValidationDone, RerankerValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            Response_haiku = Settings.llm.complete(ev.prompt.format(context_str=default_format_node_batch_fn(ev.batch_nodes), query_str=ev.query))\n",
    "            match = re.search(r'\\[(.*?)\\]', Response_haiku.text)\n",
    "            if match:\n",
    "                output = list(map(int, match.group(1).split(',')))\n",
    "            else:\n",
    "                output = []\n",
    "            new_nodes = []\n",
    "            for i in output:\n",
    "                if i-1 < len(ev.batch_nodes):\n",
    "                    new_nodes.append(ev.batch_nodes[i-1])\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return RerankerValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        rerank_nodes = new_nodes + ev.rerank_nodes\n",
    "        # if reranked nodes are less than min chunks, return to the state of batch event, otherwise output the reranked nodes.\n",
    "        if len(rerank_nodes) > self.min_chunk:\n",
    "            print(\"Reranked nodes are extracted.\")\n",
    "            if ev.neg_context == '':\n",
    "                return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                return RerankerValidationDone(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "        else:\n",
    "            return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # prepare the prompt to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_prompt(\n",
    "        self, ctx: Context, ev: Union[RerankerValidationDone, FilterValidationErrorEvent]\n",
    "    ) -> Union[StopEvent, FilterDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        rerank_nodes = ev.rerank_nodes\n",
    "\n",
    "        if isinstance(ev, RerankerValidationDone):\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, FilterValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            if current_retries >= self.max_retries:\n",
    "                if len(rerank_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            reflection_prompt = NEGATIVE_FILTER_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        prompt = NEGATIVE_FILTER_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return FilterDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # call LLM to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_validate(\n",
    "        self, ctx: Context, ev: FilterDone\n",
    "    ) -> Union[StopEvent, FilterValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            Response_haiku = Settings.llm.complete(ev.prompt.format(context_str=default_format_node_batch_fn(ev.rerank_nodes), query_str=ev.query, contexte_negatif=ev.neg_context))\n",
    "            match = re.search(r'\\[(.*?)\\]', Response_haiku.text)\n",
    "            if match:\n",
    "                output = list(map(int, match.group(1).split(',')))\n",
    "            else:\n",
    "                output = []\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return FilterValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        new_nodes = []\n",
    "        for i in output:\n",
    "            new_nodes.append(ev.rerank_nodes[i-1])\n",
    "        print(\"Reranked nodes are filtered.\")\n",
    "        current_batch = ctx.data.get(\"batch\", 0)\n",
    "        if len(new_nodes) > self.min_chunk:\n",
    "            return StopEvent(result=new_nodes)\n",
    "        else:\n",
    "            if current_batch >= self.batch_size:\n",
    "                if len(new_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=new_nodes)\n",
    "            else:\n",
    "                return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f079352e-f4e8-4182-b91f-d9943baaf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.indices.keyword_table import KeywordTableGPTRetriever\n",
    "from typing import List\n",
    "import threading\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableGPTRetriever,\n",
    "        embed_model: BedrockEmbedding,\n",
    "        old_queries: List = [],\n",
    "        mode: str = \"OR\",\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self.embed_model = embed_model\n",
    "        self.reranker = RerankerFlow(vector_retriever=vector_retriever)\n",
    "        self.old_queries = old_queries\n",
    "        index_feedback = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"finetune/vector_persist_800_feedback\"))\n",
    "        self.feedback_retriever = VectorIndexRetriever(index=index_feedback, similarity_top_k=1)\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async not activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = self._weighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = await self._aweighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        feedback_nodes = self.feedback_retriever.retrieve(query_bundle)\n",
    "        if feedback_nodes[0].score > 0.8 : \n",
    "            neg_context = feedback_nodes[0].metadata[\"message\"]\n",
    "        else:\n",
    "            neg_context = ''\n",
    "        vector_nodes = await self.reranker.run(query=query_bundle.query_str, nodes=vector_nodes_full, neg_context=neg_context)\n",
    "        if vector_nodes == None:\n",
    "            return keyword_nodes\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    def replace_abbreviations(self, question, keyword_nodes):\n",
    "        \"\"\"\n",
    "        Replaces abbreviations in the lexique with their complete names.\n",
    "        Assumes abbreviations are separated by either two spaces or one space + one punctuation mark.\n",
    "        \"\"\"\n",
    "        lexique = {}\n",
    "        for node in keyword_nodes:\n",
    "            keyword = node.node.metadata['keyword']\n",
    "            keyword_complete = node.text.split(': ')[1]\n",
    "            lexique[keyword] = keyword_complete\n",
    "\n",
    "        words = question.split()\n",
    "        result = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in lexique:\n",
    "                result.append(lexique[word])\n",
    "                result.append('('+word+')')\n",
    "            elif word[:-1] in lexique:\n",
    "                result.append(lexique[word[:-1]])\n",
    "                result.append('('+word[:-1]+')')\n",
    "                result.append(word[-1])\n",
    "            elif word[1:] in lexique:\n",
    "                result.append(word[0])\n",
    "                result.append(lexique[word[1:]])\n",
    "                result.append('('+word[1:]+')')\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def _get_query_embedding_threaded(self, query):\n",
    "        \"\"\"\n",
    "        Get the query embedding using a separate thread.\n",
    "        \"\"\"\n",
    "        result_queue = Queue()  # Create a queue to store the result\n",
    "        thread = threading.Thread(target=self._get_query_embedding_async_thread, args=(query, result_queue))\n",
    "        thread.start()\n",
    "        thread.join()  # Wait for the thread to finish\n",
    "        return result_queue.get()  # Retrieve the result from the queue\n",
    "\n",
    "    def _get_query_embedding_async_thread(self, query, result_queue):\n",
    "        \"\"\"\n",
    "        Helper method to run the asynchronous call in a separate thread.\n",
    "        \"\"\"\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(self.embed_model.aget_query_embedding(query))\n",
    "        loop.close()\n",
    "        result_queue.put(result)  # Put the result in the queue\n",
    "    \n",
    "    def _weighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = self._get_query_embedding_threaded(new_query_str)\n",
    "        print(current_embedding)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(self._get_query_embedding_threaded(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding\n",
    "    async def _aweighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = await self.embed_model.aget_query_embedding(new_query_str)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(await self.embed_model.aget_query_embedding(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbb53b0c-bce8-45f6-b751-6e96f2683226",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index_doc, similarity_top_k=30)\n",
    "keyword_retriever = KeywordTableGPTRetriever(index=index_keyword, max_keywords_per_query = 20, keyword_extract_template=KEYWORD_EXTRACT_TEMPLATE)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever, embed_model=Settings.embed_model, old_queries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff3896f0-7896-486e-ab77-8eeccad7e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "qa_template = PromptTemplate(DEFAULT_QUERY_PROMPT_TMPL)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=custom_retriever,\n",
    "                                              text_qa_template=qa_template,\n",
    "                                              use_async=True,\n",
    "                                              response_mode = ResponseMode.SIMPLE_SUMMARIZE,\n",
    "                                              streaming=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe69373d-c496-4955-a91a-adffb552476e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Reranked nodes are extracted.\n",
      "Step reranker_prompt produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event StopEvent\n",
      "========================================\n",
      "Selon les informations contextuelles fournies, pour faire une liaison détente/comptage pour alimenter une chaufferie, les tubes en acier doivent être conformes aux normes suivantes :\n",
      "\n",
      "- Les tubes en acier noirs ou galvanisés doivent être conformes aux normes NF EN 10216-1, NF A 49-115, NF A 49-141, NF EN 10217-1 et NF A 49-145.\n",
      "- Le soudage électrique sous atmosphère neutre ou le soudage oxyacétylénique (si l'épaisseur du tube est inférieure ou égale à 3,2 mm) sont autorisés pour l'assemblage de ces tubes.\n",
      "- Le soudo-brasage est également autorisé pour l'assemblage de ces tubes.\n",
      "- L'emploi de raccords mécaniques ou vissés est toléré dans certains cas, à condition qu'ils soient conformes aux normes spécifiées.\n",
      "========================================\n",
      "Execution Time: 23.93528389930725 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a85483d-b652-4db4-8b47-f1b05291baa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "========================================\n",
      "Le contexte fourni ne mentionne pas d'achats immobilisés pour le biométhane. Les informations se concentrent plutôt sur les spécifications techniques et le fonctionnement des postes d'injection de biométhane sur le réseau de distribution de gaz naturel. Le texte décrit en détail les différents équipements et systèmes qui composent ces postes, tels que :\n",
      "\n",
      "- Le système d'odorisation du biométhane, avec un dispositif d'injection de THT et un mélangeur statique.\n",
      "- Les équipements de mesure et de régulation, comme les capteurs de pression et de température, le compteur, la vanne de régulation, etc.\n",
      "- Les vannes de sécurité et les clapets anti-retour pour assurer la sécurité du réseau.\n",
      "- Le local électrique avec l'automate et les systèmes de supervision et de télétransmission.\n",
      "- Le local d'analyse de la qualité du biométhane.\n",
      "\n",
      "Le texte ne mentionne pas spécifiquement d'achats immobilisés liés au biométhane. Il se concentre plutôt sur les caractéristiques techniques et le fonctionnement des postes d'injection de biométhane sur le réseau de distribution de gaz naturel.\n",
      "========================================\n",
      "Execution Time: 27.839654445648193 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quels sont les achats immobilisés pour le biométhane ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "626a178e-b398-48cd-a8eb-a2cc8983f170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event StopEvent\n",
      "========================================\n",
      "CICM signifie \"Conduite d'Immeuble, Conduite Montante\". C'est un terme utilisé pour désigner les canalisations de gaz qui alimentent les immeubles, notamment la conduite d'immeuble située à l'intérieur du bâtiment et la conduite montante qui traverse les différents étages.\n",
      "========================================\n",
      "Execution Time: 16.471271991729736 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Que veut dire CICM ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3d61aaa-c923-48ce-adfd-186e768a0692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event StopEvent\n",
      "========================================\n",
      "Oui, dans le cadre de la surveillance des réseaux, l'opérateur doit surveiller les branchements. Selon les informations fournies :\n",
      "\n",
      "- L'opérateur doit surveiller (à pied ou en véhicule) et maintenir le réseau selon des procédures documentées, préétablies et systématiques, pour garantir la sécurité des personnes et des biens.\n",
      "\n",
      "- Le programme de surveillance et de maintenance prévoit des opérations portant sur l'ensemble des ouvrages composant le réseau, y compris les branchements.\n",
      "\n",
      "- L'opérateur doit adapter le programme de surveillance à la nature, l'environnement et l'historique du réseau, et classifier les fuites détectées selon une procédure tenant compte de la pression de service et des caractéristiques géographiques.\n",
      "\n",
      "- La politique locale de surveillance doit intégrer la surveillance :\n",
      "1- Des réseaux en acier non protégé, avec une périodicité au moins égale à celle des réseaux sous protection cathodique et dans tous les cas au moins tous les 2 ans.\n",
      "2- Des réseaux neufs, au plus tard 12 mois après leur mise en service, y compris les branchements et notamment leurs parties hors sol.\n",
      "3- Des fuites de classe 3, avec une surveillance périodique.\n",
      "4- Des points singuliers du réseau, avec un inventaire et une analyse des actes de surveillance réalisés.\n",
      "========================================\n",
      "Execution Time: 21.154531955718994 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcd0db-46f9-499a-a33e-70e6ab111924",
   "metadata": {},
   "source": [
    "## Claude haiku -- sans sousquestion  -- 11.35 seconds / 4 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ac53c0-ebed-4e0b-8446-af1b523ead16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    ")\n",
    "import re\n",
    "from llama_index.core.indices.utils import (\n",
    "    default_format_node_batch_fn,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.output_parsers.pydantic import PydanticOutputParser\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional, Type, Union\n",
    "\n",
    "SUB_QUESTION_PROMPT_STR = \"\"\"\n",
    "Vous êtes un assistant utile qui génère plusieurs requêtes de recherche basées sur une seule requête d'entrée. \n",
    "Générer 3 requêtes de recherche, une sur chaque ligne, liées à la requête d'entrée suivante:\n",
    "Requête: {query}\n",
    "Requêtes:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_SELECT_PROMPT_TMPL = \"\"\"\n",
    "Une liste de documents est présentée ci-dessous. Chaque document est accompagné d'un numéro et d'un résumé du document. Une question est également fournie.\n",
    "Donne-moi uniquement les numéros des documents pertinents à la question en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "...\n",
    "\n",
    "Document 10:\n",
    "<résumé du document 10>\n",
    "\n",
    "Question: <question>\n",
    "Tableau des nombres des documents pertinents: [2, 4, 5, 6]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "{context_str}\n",
    "Question: {query_str}\n",
    "Tableau des nombres des documents pertinents:\n",
    "\"\"\"\n",
    "\n",
    "CHOICE_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents pertinents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_PROMPT_TMPL = \"\"\"\n",
    "Voici une liste de contextes pertinents pour une requête donnée. Vous devez filtrer cette liste pour exclure les contextes qui correspondent à un contexte négatif spécifique.\n",
    "Donne-moi uniquement les numéros des documents en utilisant le format du tableau.\n",
    "Voici quelques exemples: \n",
    "Requête principale : <question>\n",
    "Contexte négatif : <contexte_negatif>\n",
    "Liste des contextes :\n",
    "Document 1:\n",
    "<résumé du document 1>\n",
    "\n",
    "Document 2:\n",
    "<résumé du document 2>\n",
    "\n",
    "Document 3:\n",
    "<résumé du document 10>\n",
    "\n",
    "Document 4:\n",
    "<résumé du document 10>\n",
    "\n",
    "Tableau des nombres des documents: [1, 2, 3]\n",
    "\n",
    "Essayons ceci maintenant :\n",
    "\n",
    "Requête principale : {query_str}\n",
    "Contexte négatif : {contexte_negatif}\n",
    "Liste des contextes : {contexte}\n",
    "Tableau des nombres des documents:\n",
    "\"\"\"\n",
    "\n",
    "NEGATIVE_FILTER_REFLECTION_PROMPT_STR = \"\"\"\n",
    "Cela a provoqué l'erreur : {error}\n",
    "\n",
    "Réessayez, la réponse doit contenir uniquement les numéros des documents.\n",
    "Ne répétez pas le schéma.\n",
    "\"\"\"\n",
    "\n",
    "class SubQuestionEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "\n",
    "class RerankerValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerBatchEvent(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "    batch_nodes: list\n",
    "\n",
    "class RerankerValidationDone(Event):\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class FilterDone(Event):\n",
    "    prompt: str\n",
    "    query: str\n",
    "    neg_context: str\n",
    "    nodes: list\n",
    "    rerank_nodes: list\n",
    "\n",
    "class RerankerFlow(Workflow):\n",
    "    def __init__(self, vector_retriever: VectorIndexRetriever, timeout = 120, verbose = True):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.batch_size = 3 # the divided size to rerank all the nodes \n",
    "        self.max_retries = 3 # the max retries for the prompt\n",
    "        self.min_chunk = 10 # the minimum size of the chunks after the rerank\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "\n",
    "    # the state to generate sub questions and put inside nodes\n",
    "    @step()\n",
    "    async def sub_question(\n",
    "        self, ev: StartEvent\n",
    "    ) -> SubQuestionEvent:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        # subquestions = []\n",
    "        # try:\n",
    "        #     subquestions_response = Settings.llm.complete(SUB_QUESTION_PROMPT_STR.format(query=query))\n",
    "        #     subquestions = subquestions_response.text.split(\"\\n\")\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        # if len(subquestions) > 3:\n",
    "        #     nodes1 = self.vector_retriever.retrieve(subquestions[-3])\n",
    "        #     nodes2 = self.vector_retriever.retrieve(subquestions[-2])\n",
    "        #     nodes3 = self.vector_retriever.retrieve(subquestions[-1])\n",
    "        #     one_third_index = int(len(nodes)/3)\n",
    "        #     two_third_index = int(len(nodes)/3*2)\n",
    "        #     nodes_ids = []\n",
    "        #     for node in nodes:\n",
    "        #         nodes_ids.append(node.id_)\n",
    "        #     indicator1 = 0\n",
    "        #     indicator2 = 0\n",
    "        #     indicator3 = 0\n",
    "        #     for i in range(len(nodes1)):\n",
    "        #         if nodes1[i].id_ not in nodes_ids:\n",
    "        #             indicator1 += 1\n",
    "        #             nodes_ids.append(nodes1[i].id_)\n",
    "        #             if indicator1 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes1[i])\n",
    "        #             elif indicator1 == 5:\n",
    "        #                 nodes.append(nodes1[i])\n",
    "        #             elif indicator1 == 6:\n",
    "        #                 nodes.append(nodes1[i])\n",
    "        #                 break\n",
    "        #     for i in range(len(nodes2)):\n",
    "        #         if nodes2[i].id_ not in nodes_ids:\n",
    "        #             indicator2 += 1\n",
    "        #             nodes_ids.append(nodes2[i].id_)\n",
    "        #             if indicator2 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes2[i])\n",
    "        #             elif indicator2 == 5:\n",
    "        #                 nodes.append(nodes2[i])\n",
    "        #             elif indicator2 == 6:\n",
    "        #                 nodes.append(nodes2[i])\n",
    "        #                 break\n",
    "        #     for i in range(len(nodes3)):\n",
    "        #         if nodes3[i].id_ not in nodes_ids:\n",
    "        #             indicator3 += 1\n",
    "        #             nodes_ids.append(nodes3[i].id_)\n",
    "        #             if indicator3 == 1:\n",
    "        #                 nodes.insert(one_third_index, nodes3[i])\n",
    "        #             elif indicator3 == 2:\n",
    "        #                 nodes.insert(one_third_index, nodes3[i])\n",
    "        #             elif indicator3 == 3:\n",
    "        #                 nodes.insert(two_third_index, nodes3[i])\n",
    "        #             elif indicator3 == 4:\n",
    "        #                 nodes.insert(two_third_index, nodes3[i])\n",
    "        #             elif indicator3 == 5:\n",
    "        #                 nodes.append(nodes3[i])\n",
    "        #             elif indicator3 == 6:\n",
    "        #                 nodes.append(nodes3[i])\n",
    "        #                 break\n",
    "        return SubQuestionEvent(query=query, neg_context=neg_context, nodes=nodes)\n",
    "\n",
    "    # the state to generate prompt for reranking\n",
    "    @step(pass_context=True)\n",
    "    async def reranker_prompt(\n",
    "        self, ctx: Context, ev: Union[SubQuestionEvent, RerankerValidationErrorEvent, RerankerBatchEvent]\n",
    "    ) -> Union[StopEvent, RerankerDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        window = int(len(nodes)/3)\n",
    "\n",
    "        if isinstance(ev, SubQuestionEvent):\n",
    "            rerank_nodes = []\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, RerankerBatchEvent):\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            # iterate the nodes based on batch size\n",
    "            if current_batch >= self.batch_size:\n",
    "                print(\"Reranked nodes are extracted.\")\n",
    "                # if no negative context, output reranked nodes. otherwise, go the state of filter\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"batch\"] = current_batch + 1\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = \"\"\n",
    "        # prompt error event\n",
    "        elif isinstance(ev, RerankerValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            rerank_nodes = ev.rerank_nodes\n",
    "            if current_retries >= self.max_retries:\n",
    "                if neg_context == '':\n",
    "                    if len(rerank_nodes) == 0:\n",
    "                        return StopEvent(result=None)\n",
    "                    else:\n",
    "                        return StopEvent(result=rerank_nodes)\n",
    "                else:\n",
    "                    return RerankerValidationDone(query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            current_batch = ctx.data.get(\"batch\", 0)\n",
    "            batch_nodes = nodes[window*current_batch:window*(current_batch+1)]\n",
    "            reflection_prompt = CHOICE_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        \n",
    "        prompt = CHOICE_SELECT_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return RerankerDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes, batch_nodes=batch_nodes)\n",
    "\n",
    "    # call LLM to rerank nodes\n",
    "    @step()\n",
    "    async def rerank_validate(\n",
    "        self, ev: RerankerDone\n",
    "    ) -> Union[StopEvent, RerankerValidationDone, RerankerValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            Response_haiku = Settings.llm.complete(ev.prompt.format(context_str=default_format_node_batch_fn(ev.batch_nodes), query_str=ev.query))\n",
    "            match = re.search(r'\\[(.*?)\\]', Response_haiku.text)\n",
    "            if match:\n",
    "                output = list(map(int, match.group(1).split(',')))\n",
    "            else:\n",
    "                output = []\n",
    "            new_nodes = []\n",
    "            for i in output:\n",
    "                if i-1 < len(ev.batch_nodes):\n",
    "                    new_nodes.append(ev.batch_nodes[i-1])\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return RerankerValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        rerank_nodes = new_nodes + ev.rerank_nodes\n",
    "        # if reranked nodes are less than min chunks, return to the state of batch event, otherwise output the reranked nodes.\n",
    "        if len(rerank_nodes) > self.min_chunk:\n",
    "            print(\"Reranked nodes are extracted.\")\n",
    "            if ev.neg_context == '':\n",
    "                return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                return RerankerValidationDone(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "        else:\n",
    "            return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # prepare the prompt to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_prompt(\n",
    "        self, ctx: Context, ev: Union[RerankerValidationDone, FilterValidationErrorEvent]\n",
    "    ) -> Union[StopEvent, FilterDone]:\n",
    "        query = ev.query\n",
    "        nodes = ev.nodes\n",
    "        neg_context = ev.neg_context\n",
    "        rerank_nodes = ev.rerank_nodes\n",
    "\n",
    "        if isinstance(ev, RerankerValidationDone):\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, FilterValidationErrorEvent):\n",
    "            current_retries = ctx.data.get(\"prompt_retries\", 0)\n",
    "            if current_retries >= self.max_retries:\n",
    "                if len(rerank_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=rerank_nodes)\n",
    "            else:\n",
    "                ctx.data[\"prompt_retries\"] = current_retries + 1\n",
    "            reflection_prompt = NEGATIVE_FILTER_REFLECTION_PROMPT_STR.format(error=ev.error)\n",
    "        prompt = NEGATIVE_FILTER_PROMPT_TMPL\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "        return FilterDone(prompt=prompt, query=query, neg_context=neg_context, nodes=nodes, rerank_nodes=rerank_nodes)\n",
    "\n",
    "    # call LLM to filter the reranked nodes\n",
    "    @step(pass_context=True)\n",
    "    async def filter_validate(\n",
    "        self, ctx: Context, ev: FilterDone\n",
    "    ) -> Union[StopEvent, FilterValidationErrorEvent, RerankerBatchEvent]:\n",
    "        try:\n",
    "            Response_haiku = Settings.llm.complete(ev.prompt.format(context_str=default_format_node_batch_fn(ev.rerank_nodes), query_str=ev.query, contexte_negatif=ev.neg_context))\n",
    "            match = re.search(r'\\[(.*?)\\]', Response_haiku.text)\n",
    "            if match:\n",
    "                output = list(map(int, match.group(1).split(',')))\n",
    "            else:\n",
    "                output = []\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            return FilterValidationErrorEvent(\n",
    "                error=str(e), query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=ev.rerank_nodes\n",
    "            )\n",
    "        new_nodes = []\n",
    "        for i in output:\n",
    "            new_nodes.append(ev.rerank_nodes[i-1])\n",
    "        print(\"Reranked nodes are filtered.\")\n",
    "        current_batch = ctx.data.get(\"batch\", 0)\n",
    "        if len(new_nodes) > self.min_chunk:\n",
    "            return StopEvent(result=new_nodes)\n",
    "        else:\n",
    "            if current_batch >= self.batch_size:\n",
    "                if len(new_nodes) == 0:\n",
    "                    return StopEvent(result=None)\n",
    "                else:\n",
    "                    return StopEvent(result=new_nodes)\n",
    "            else:\n",
    "                return RerankerBatchEvent(query=ev.query, neg_context=ev.neg_context, nodes=ev.nodes, rerank_nodes=new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeec5c17-4f1f-4b8f-9e95-ccb1c30cf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
    "from llama_index.core.indices.keyword_table import KeywordTableGPTRetriever\n",
    "from typing import List\n",
    "import threading\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableGPTRetriever,\n",
    "        embed_model: BedrockEmbedding,\n",
    "        old_queries: List = [],\n",
    "        mode: str = \"OR\",\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        self.embed_model = embed_model\n",
    "        self.reranker = RerankerFlow(vector_retriever=vector_retriever)\n",
    "        self.old_queries = old_queries\n",
    "        index_feedback = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=\"finetune/vector_persist_800_feedback\"))\n",
    "        self.feedback_retriever = VectorIndexRetriever(index=index_feedback, similarity_top_k=1)\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async not activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = self._weighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        print(\"async activated.\")\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "        new_query_str = self.replace_abbreviations(query_bundle.query_str, keyword_nodes)\n",
    "        query_bundle.embedding = await self._aweighted_aggre_embedding(new_query_str)\n",
    "        vector_nodes_full = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        feedback_nodes = self.feedback_retriever.retrieve(query_bundle)\n",
    "        if feedback_nodes[0].score > 0.8 : \n",
    "            neg_context = feedback_nodes[0].metadata[\"message\"]\n",
    "        else:\n",
    "            neg_context = ''\n",
    "        vector_nodes = await self.reranker.run(query=query_bundle.query_str, nodes=vector_nodes_full, neg_context=neg_context)\n",
    "        if vector_nodes == None:\n",
    "            return keyword_nodes\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes\n",
    "\n",
    "    def replace_abbreviations(self, question, keyword_nodes):\n",
    "        \"\"\"\n",
    "        Replaces abbreviations in the lexique with their complete names.\n",
    "        Assumes abbreviations are separated by either two spaces or one space + one punctuation mark.\n",
    "        \"\"\"\n",
    "        lexique = {}\n",
    "        for node in keyword_nodes:\n",
    "            keyword = node.node.metadata['keyword']\n",
    "            keyword_complete = node.text.split(': ')[1]\n",
    "            lexique[keyword] = keyword_complete\n",
    "\n",
    "        words = question.split()\n",
    "        result = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in lexique:\n",
    "                result.append(lexique[word])\n",
    "                result.append('('+word+')')\n",
    "            elif word[:-1] in lexique:\n",
    "                result.append(lexique[word[:-1]])\n",
    "                result.append('('+word[:-1]+')')\n",
    "                result.append(word[-1])\n",
    "            elif word[1:] in lexique:\n",
    "                result.append(word[0])\n",
    "                result.append(lexique[word[1:]])\n",
    "                result.append('('+word[1:]+')')\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def _get_query_embedding_threaded(self, query):\n",
    "        \"\"\"\n",
    "        Get the query embedding using a separate thread.\n",
    "        \"\"\"\n",
    "        result_queue = Queue()  # Create a queue to store the result\n",
    "        thread = threading.Thread(target=self._get_query_embedding_async_thread, args=(query, result_queue))\n",
    "        thread.start()\n",
    "        thread.join()  # Wait for the thread to finish\n",
    "        return result_queue.get()  # Retrieve the result from the queue\n",
    "\n",
    "    def _get_query_embedding_async_thread(self, query, result_queue):\n",
    "        \"\"\"\n",
    "        Helper method to run the asynchronous call in a separate thread.\n",
    "        \"\"\"\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        result = loop.run_until_complete(self.embed_model.aget_query_embedding(query))\n",
    "        loop.close()\n",
    "        result_queue.put(result)  # Put the result in the queue\n",
    "    \n",
    "    def _weighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = self._get_query_embedding_threaded(new_query_str)\n",
    "        print(current_embedding)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = self._get_query_embedding_threaded(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(self._get_query_embedding_threaded(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding\n",
    "    async def _aweighted_aggre_embedding(self, new_query_str):\n",
    "        \"\"\"\n",
    "        A weighted aggregation embedding method which can integrate the preivous queries in the current query.\n",
    "        \"\"\"\n",
    "        current_embedding = await self.embed_model.aget_query_embedding(new_query_str)\n",
    "        if len(self.old_queries) == 0:\n",
    "            return current_embedding\n",
    "        elif len(self.old_queries) == 1:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i])\n",
    "            return new_embedding\n",
    "        else:\n",
    "            first_embedding = await self.embed_model.aget_query_embedding(self.old_queries[0])\n",
    "            other_embeddings = []\n",
    "            for query in self.old_queries[1:]:\n",
    "                other_embeddings.append(await self.embed_model.aget_query_embedding(query))\n",
    "            other_embedding = list(np.array(other_embeddings).mean(axis=0))\n",
    "            new_embedding = []\n",
    "            for i in range(len(current_embedding)):\n",
    "                new_embedding.append(0.8 * current_embedding[i] + 0.2 * first_embedding[i] + 0.1 * other_embedding[i])\n",
    "            return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83569174-2ddd-4fed-971b-e5bd793b5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index_doc, similarity_top_k=30)\n",
    "keyword_retriever = KeywordTableGPTRetriever(index=index_keyword, max_keywords_per_query = 20, keyword_extract_template=KEYWORD_EXTRACT_TEMPLATE)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever, embed_model=Settings.embed_model, old_queries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "706fc8e9-735f-4061-8a9d-c5afb5e5e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "qa_template = PromptTemplate(DEFAULT_QUERY_PROMPT_TMPL)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=custom_retriever,\n",
    "                                              text_qa_template=qa_template,\n",
    "                                              use_async=True,\n",
    "                                              response_mode = ResponseMode.SIMPLE_SUMMARIZE,\n",
    "                                              streaming=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "652d7f28-9bac-48b1-83bf-30a9c5d1fe21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async not activated.\n",
      "[0.10205078125, -0.279296875, 0.00970458984375, -0.005462646484375, -0.302734375, 0.40234375, 0.07763671875, -0.00020503997802734375, -0.369140625, 0.29296875, -0.484375, 0.107421875, -0.1337890625, 0.058349609375, 0.408203125, 0.3515625, 0.38671875, -0.1083984375, 0.5234375, 0.00384521484375, 0.09716796875, -0.91796875, 0.28515625, 0.2001953125, -0.2021484375, 0.39453125, 0.1767578125, -0.057373046875, 0.054443359375, -0.4921875, -0.369140625, -0.2255859375, -0.119140625, -1.171875, 0.7109375, 0.1689453125, -0.05224609375, -0.2490234375, -0.37109375, 0.0693359375, 0.0927734375, 0.419921875, 0.154296875, -0.57421875, 0.2314453125, -0.02734375, -0.006500244140625, 0.279296875, 0.3671875, 0.14453125, -0.27734375, 0.36328125, 0.19921875, 0.2060546875, -0.298828125, -0.06982421875, 0.26953125, -0.18359375, 0.61328125, 0.2294921875, -0.61328125, 0.2294921875, 0.55859375, -0.330078125, 0.318359375, 0.10009765625, -0.1787109375, -0.427734375, -0.39453125, 0.287109375, -0.330078125, -0.1455078125, 0.0107421875, 0.076171875, 0.0986328125, 0.000762939453125, 0.12109375, -0.0810546875, 0.19140625, -0.1142578125, 0.0556640625, 0.224609375, 0.43359375, 0.625, -0.5390625, 0.1572265625, -0.181640625, 0.123046875, 0.00012302398681640625, 0.07080078125, 0.0135498046875, 0.54296875, -0.41015625, 0.6171875, 0.435546875, 0.00390625, 0.392578125, 1.2421875, 0.291015625, 0.365234375, -0.3203125, -0.0791015625, 0.1611328125, -1.0703125, 0.263671875, -0.076171875, 0.275390625, -0.1123046875, -0.07080078125, -0.283203125, 0.047119140625, -0.07958984375, 0.01080322265625, -0.462890625, -0.79296875, -0.69921875, 0.2421875, 0.0859375, -0.412109375, 0.44921875, -0.625, 0.349609375, -0.50390625, 0.08642578125, 0.0419921875, -0.80078125, 0.306640625, -0.69140625, -0.228515625, 0.6640625, 0.75, -0.431640625, -0.21875, -0.0103759765625, 0.1171875, -0.400390625, -0.80078125, 0.185546875, -0.251953125, -0.62109375, -0.0267333984375, 0.474609375, -0.1552734375, -0.404296875, -0.1787109375, 0.392578125, 0.2197265625, -0.66015625, -0.80859375, 0.01190185546875, 0.2099609375, -0.35546875, -0.73828125, 0.171875, -0.44921875, 0.1435546875, 0.81640625, -0.1875, 0.78515625, -0.2734375, 0.39453125, -0.0859375, -0.06591796875, -0.6015625, -0.83203125, 0.39453125, -0.076171875, -0.04248046875, 0.447265625, 0.08203125, 0.30859375, -0.80859375, 0.50390625, 0.515625, 0.08642578125, 0.1796875, 1.265625, 0.3203125, -0.111328125, 0.15234375, -0.431640625, -0.23828125, -0.10693359375, -0.625, -0.031494140625, 0.462890625, 0.421875, 0.08740234375, -0.2353515625, 0.28515625, -0.35546875, -0.4609375, 0.158203125, -1.3125, 0.87890625, -0.2255859375, 0.09521484375, 0.279296875, -0.494140625, 0.0194091796875, 0.034912109375, 0.07568359375, 0.69140625, 0.0142822265625, 0.87109375, -0.400390625, 0.0791015625, 0.060546875, -1.1015625, 0.10693359375, -0.12890625, 0.07421875, 0.60546875, -0.66015625, 0.005462646484375, 0.48828125, 0.01226806640625, 0.2421875, -0.306640625, 0.5234375, -0.07373046875, 0.1455078125, -0.5390625, -0.51171875, -0.357421875, -0.267578125, 0.484375, -0.80078125, 0.1142578125, 0.0830078125, -0.08056640625, -0.2138671875, -0.7578125, -0.62890625, -0.46484375, -0.33984375, -0.2451171875, 0.0240478515625, 0.036865234375, -0.27734375, -0.279296875, -0.3671875, 0.625, 0.302734375, -0.1982421875, -0.041259765625, 0.12451171875, 0.1015625, -0.33984375, 0.0576171875, -0.345703125, -0.51953125, 1.0, -0.10546875, 0.10986328125, 0.294921875, -1.0859375, -0.498046875, 0.056884765625, 0.3046875, 0.1513671875, 0.73046875, -0.169921875, 0.4140625, 0.2021484375, 0.181640625, -0.296875, -0.0927734375, -0.26171875, -0.007781982421875, 0.00011587142944335938, 0.4765625, 0.2451171875, 0.1005859375, 0.009765625, -0.1484375, -0.18359375, -0.26953125, 0.62890625, 0.373046875, -0.1435546875, -0.208984375, 0.30078125, 0.34375, -0.453125, 0.1708984375, -0.1240234375, 0.365234375, 0.5546875, 0.21484375, 0.294921875, 0.1640625, 0.07177734375, 0.478515625, -0.06982421875, 0.43359375, -0.267578125, -0.07958984375, -0.484375, 0.5, 0.69921875, -0.53515625, 0.1884765625, 0.59375, -0.158203125, 0.07177734375, -0.08056640625, 0.28515625, 0.0184326171875, -0.298828125, 0.40234375, 0.33203125, 0.19921875, 0.1396484375, -0.421875, 0.11181640625, 0.7109375, 0.3125, -0.421875, 0.0673828125, -0.419921875, 0.205078125, 0.10009765625, 0.1787109375, -0.25, 0.8515625, 0.66015625, -0.296875, 0.447265625, 0.0751953125, -0.01031494140625, 0.265625, -0.5234375, 0.248046875, 0.66015625, -0.2060546875, -0.376953125, -0.421875, -0.71875, -0.205078125, -0.2275390625, 0.005340576171875, -0.1533203125, 0.59375, 0.0693359375, 0.166015625, -0.0028533935546875, -0.251953125, -0.474609375, 0.267578125, 0.1240234375, 0.083984375, 0.255859375, -0.32421875, -0.25390625, 0.28515625, -0.09033203125, 0.1337890625, -0.359375, 0.287109375, -0.0281982421875, 0.00250244140625, 0.419921875, 0.828125, -0.42578125, -0.046875, -0.1640625, 0.005096435546875, -0.439453125, -0.7890625, 0.2578125, 0.7578125, 0.515625, -0.2890625, -0.0181884765625, 1.0859375, -0.0693359375, -0.16796875, -0.08935546875, 0.703125, -0.447265625, 0.43359375, 0.24609375, 0.1064453125, 0.578125, -0.275390625, -0.29296875, 0.326171875, -0.078125, 0.79296875, 0.33203125, 0.220703125, -0.3515625, 0.265625, 0.2021484375, 0.043701171875, -0.263671875, -0.33984375, 0.1865234375, 0.384765625, -0.30078125, 0.53125, 0.54296875, 0.5234375, -0.734375, 0.87109375, -0.79296875, -0.404296875, 0.1005859375, -0.35546875, -0.125, -0.212890625, 0.1328125, 0.2890625, 0.0062255859375, 0.1787109375, 0.0245361328125, 0.271484375, -0.1376953125, 0.7578125, -0.359375, 0.83984375, -0.140625, 0.384765625, 0.6484375, 0.1171875, 0.15625, 0.10498046875, -0.33984375, -0.072265625, 0.2392578125, -0.1650390625, -0.4140625, 0.1142578125, 0.185546875, -0.039794921875, 0.341796875, 0.1181640625, 0.078125, -0.3515625, -0.078125, 0.016357421875, -0.15234375, -0.46875, -0.1337890625, -0.51953125, 0.17578125, 0.2001953125, -0.10400390625, -0.41015625, -0.057861328125, -0.5078125, -0.12353515625, -0.1494140625, 0.228515625, -0.3046875, -0.373046875, -0.5546875, 0.1826171875, -0.09716796875, -0.6328125, 0.00060272216796875, -0.171875, -0.2099609375, 0.043212890625, 0.46875, 0.78515625, -0.23828125, -0.890625, -0.1484375, -0.365234375, -0.2041015625, -0.8046875, 0.28515625, -0.51953125, 0.2578125, 0.0262451171875, 0.3046875, -0.0859375, -0.65625, -0.0162353515625, 0.294921875, 0.1767578125, -0.103515625, -0.201171875, 0.27734375, -0.390625, 0.5703125, 0.25, 0.58203125, -0.11669921875, -0.1181640625, 0.05810546875, 0.4453125, -0.404296875, 0.0615234375, 3.24249267578125e-05, 0.2021484375, -0.1845703125, 0.423828125, 0.1416015625, -0.01312255859375, 0.12158203125, -0.09130859375, 0.7109375, 0.71484375, 0.33203125, 0.1669921875, 0.1279296875, 0.0206298828125, 0.045166015625, 0.5625, 0.58984375, -0.4140625, -0.00040435791015625, 0.0267333984375, 0.166015625, -0.275390625, -0.1298828125, -0.050537109375, -0.50390625, -0.7578125, -0.0693359375, -0.625, 0.408203125, 0.2197265625, -0.2294921875, -0.421875, -0.9375, 0.140625, 0.6328125, 0.2890625, -0.490234375, -0.06640625, -0.30078125, 0.283203125, -0.384765625, -0.388671875, -0.2265625, -0.2265625, -0.373046875, 0.84375, 0.76171875, -0.431640625, 0.44140625, -0.466796875, -0.447265625, 0.53515625, 0.115234375, -0.373046875, -0.81640625, -0.1875, -0.546875, -0.671875, 0.447265625, -0.578125, 0.7734375, 0.4765625, -0.224609375, -0.62890625, 0.291015625, 0.34375, -0.08203125, 0.25, 0.134765625, 0.27734375, 0.0859375, 0.0019378662109375, 0.36328125, -0.0830078125, 0.36328125, -0.30078125, 0.341796875, 0.298828125, -0.2373046875, 0.279296875, 0.1953125, -0.0771484375, -0.158203125, -0.197265625, 0.1220703125, 0.55859375, 0.04541015625, 0.1083984375, 0.318359375, -0.10693359375, 0.1875, 0.2099609375, 0.0849609375, -0.388671875, -0.765625, -0.10205078125, -0.326171875, -0.1875, 0.2333984375, 0.0703125, 0.302734375, 0.388671875, 0.359375, 0.1572265625, -0.166015625, 0.3046875, 0.703125, 0.263671875, -0.97265625, 0.07275390625, 0.0294189453125, -0.96875, -0.365234375, 0.09619140625, -0.875, 0.48046875, 0.1640625, 0.53515625, -0.57421875, -0.03466796875, -0.60546875, -0.203125, -0.423828125, -1.03125, -0.60546875, -1.015625, 0.0712890625, 0.1044921875, -0.0888671875, -0.33984375, 0.0869140625, 0.1767578125, -0.2490234375, -0.09228515625, 0.337890625, 0.0654296875, -0.08154296875, -0.3515625, -0.4375, 0.359375, 0.5859375, -0.1572265625, 0.287109375, -0.038818359375, 0.5546875, 0.0177001953125, 0.0439453125, -0.90625, 0.87890625, 0.78125, 0.6953125, 0.67578125, 0.13671875, 0.23046875, 0.546875, -0.4609375, 0.08154296875, -0.09033203125, -0.1767578125, -0.75390625, -0.1865234375, 0.034912109375, 0.283203125, -0.12158203125, 0.20703125, 0.322265625, -0.1640625, -0.00762939453125, -0.06884765625, -0.1318359375, -0.08740234375, -1.140625, -0.03759765625, -0.1552734375, -0.02978515625, 0.4609375, 0.41015625, 0.025634765625, -0.05810546875, -0.322265625, -0.375, -0.265625, 0.73046875, 0.189453125, 0.0198974609375, 0.5390625, 0.1279296875, -0.400390625, 0.2060546875, 0.31640625, -0.8515625, 0.263671875, 0.0262451171875, 0.11669921875, 0.2177734375, 0.27734375, -0.048095703125, 0.11572265625, -0.00439453125, 0.52734375, 0.10302734375, 0.234375, 0.365234375, -0.1171875, -0.1337890625, -0.244140625, -0.0302734375, 0.25390625, 0.1279296875, -0.96875, 0.296875, -0.006134033203125, -0.166015625, -0.51171875, 0.28125, 0.03955078125, 0.154296875, 0.4765625, 0.0439453125, -0.07275390625, -0.3515625, 0.6328125, -0.625, 0.326171875, 0.353515625, -0.11962890625, 0.111328125, 0.3828125, 0.1865234375, 0.0908203125, 0.283203125, -0.05712890625, 0.12109375, -0.203125, 0.33984375, 0.10888671875, 0.8828125, -0.08544921875, 1.0703125, -0.294921875, 0.05078125, -0.208984375, 0.07177734375, 0.24609375, 0.357421875, -0.53515625, 0.036865234375, -0.0625, 0.259765625, -0.07080078125, -0.255859375, 0.578125, 0.384765625, 0.3203125, 0.87109375, -0.54296875, -0.08642578125, 0.0927734375, -0.70703125, 0.1826171875, 0.1806640625, -0.7421875, -0.197265625, -0.03369140625, -0.78515625, -0.02197265625, -0.498046875, 0.345703125, 1.3515625, -0.00799560546875, -0.298828125, -0.091796875, 0.30859375, -0.0478515625, 0.23828125, 0.0150146484375, -0.2236328125, -0.267578125, -0.28515625, -0.046875, 0.041259765625, -0.1396484375, 0.4609375, 0.2353515625, 0.1064453125, -0.057373046875, 0.053955078125, 0.115234375, 0.494140625, -0.4609375, -1.1484375, 0.431640625, 0.7890625, 0.546875, -0.66796875, -0.53125, -0.52734375, 0.09326171875, 0.2373046875, -0.28515625, 0.044189453125, 0.103515625, 0.41015625, -0.193359375, 0.046142578125, 0.07958984375, -0.5078125, 0.58203125, -0.82421875, -0.421875, -0.197265625, 0.271484375, 0.11572265625, -0.53515625, 0.5390625, -0.072265625, -0.0296630859375, 0.287109375, 0.796875, 0.3046875, -0.81640625, -0.421875, -0.46875, -0.05078125, 0.5546875, 0.482421875, -0.58984375, -0.71875, 0.150390625, 0.0012359619140625, 0.1455078125, -0.283203125, 0.70703125, -0.9140625, -0.236328125, -0.8125, -0.038330078125, -0.3359375, 0.2236328125, -0.2333984375, -0.63671875, 0.2353515625, -0.08251953125, 0.2373046875, -0.6484375, 0.46875, -0.1630859375, -0.19921875, -0.0771484375, -0.9609375, -0.01068115234375, -0.0272216796875, 0.2294921875, 0.1748046875, 0.294921875, -0.384765625, -0.3203125, 0.5, 0.2353515625, -0.158203125, -0.408203125, -0.2412109375, -0.421875, -0.3046875, 0.474609375, 0.12255859375, 0.796875, 0.11376953125, 0.1689453125, 0.291015625, 0.296875, 0.54296875, -0.51171875, -0.2734375, -0.423828125, -0.14453125, 0.4609375, 0.45703125, 0.1796875, -0.255859375, 0.1650390625, -0.703125, 0.1484375, 0.1279296875, -0.326171875, -1.3125, -0.236328125, -0.11474609375, -0.34765625, -0.08642578125, 0.08642578125, 1.046875, -0.0830078125, 0.1826171875, 1.2734375, -0.359375, 0.47265625, -0.051025390625, 0.57421875, 0.2275390625, 0.306640625, 0.5078125, -0.37890625, 0.7890625, 0.166015625, -1.171875, -0.263671875, 0.0159912109375, -0.0244140625, -0.11669921875, -0.1865234375, 0.7578125, -0.33984375, 0.4140625, -0.111328125, -0.6796875, -0.4140625, -0.33984375, -0.0576171875, 0.38671875, 0.51171875, 0.5390625, 0.65234375, 0.15625, 0.2578125, 0.01446533203125, -0.8203125, 0.498046875, 0.6796875, 0.34765625, 0.054931640625, 0.3046875, -0.435546875, -0.205078125, -0.099609375, -0.1005859375, 0.08056640625, 0.345703125, -0.57421875, -0.248046875, 0.7734375, 0.5078125, 0.11328125, 0.63671875, 0.5859375, -0.0003490447998046875, 0.0830078125, -0.294921875, 0.134765625, -0.134765625, 0.049072265625, 0.0927734375, -0.322265625, 0.05078125, -0.26171875, -0.50390625, 0.138671875, 0.00041961669921875, 0.38671875, -0.23828125, -0.0184326171875, -0.228515625, -0.2158203125, 0.3359375, 0.369140625, 0.265625, -0.609375, -0.05224609375, -0.359375, -0.4765625, -0.0172119140625, -0.76953125, 0.053466796875, 0.51953125, 0.34375, 0.28125, -0.4296875, 0.150390625, -0.52734375, -0.400390625, 0.25, 0.103515625, 0.609375, -0.004730224609375, 0.388671875, -0.00445556640625, -0.0615234375, 0.240234375, 0.14453125, 0.87109375, 0.146484375, -0.359375, 0.08154296875, -0.65234375, 0.1083984375, 0.30078125, -0.1953125, 0.408203125, 0.1416015625, -0.458984375, 0.376953125, 0.4765625, -0.6484375, -0.59765625, -0.056884765625, -0.1953125, 0.01019287109375, -0.57421875, -0.0281982421875, -0.4921875, 0.06201171875, 0.169921875, -0.1474609375, 0.109375, -0.251953125, 0.57421875, -0.3203125, -0.0869140625, -0.38671875, 0.58984375, 0.1796875, -0.2021484375, 0.26953125, -0.12255859375, 0.287109375, 0.018798828125, -0.251953125, 0.10546875, -0.3203125, -0.06494140625, -0.1767578125, -0.69140625, 0.02392578125, 0.0262451171875, 0.2060546875, 0.74609375, -0.064453125, -0.333984375, 0.134765625, 0.25, -0.0220947265625, -0.99609375, 0.04638671875, 0.625, -0.5625, 0.0294189453125, -0.154296875, 1.0703125, -0.6015625, 0.08447265625, 0.51953125, -0.061767578125, 0.00103759765625, 0.275390625, 0.0810546875, 0.345703125, 0.0037384033203125, -0.359375, 0.134765625, -0.70703125, -0.1162109375, 0.28515625, -0.171875, -0.0908203125, 0.404296875, -0.1484375, -0.1318359375, -0.59375, 0.337890625, 0.384765625, 0.3359375, 0.2216796875, 0.62890625, -0.1337890625, 0.326171875, -0.283203125, -0.055419921875, 0.018798828125, 0.07470703125, -0.306640625, -0.2734375, 0.333984375, -0.2578125, 0.26953125, 0.404296875, 0.3515625, 0.11328125, -0.392578125, 0.57421875, 0.10498046875, 0.0294189453125, -0.330078125, 0.2001953125, -0.039306640625, 0.486328125, -0.3203125, 0.54296875, -0.515625, -0.337890625, -0.0234375, 0.353515625, 0.2080078125, -0.267578125, -0.294921875, 0.625, -0.373046875, 0.6484375, -0.474609375, -0.91015625, -0.71484375, 0.1591796875, -0.28125, -0.1025390625, 0.76171875, 0.1162109375, 0.248046875, 0.318359375, 0.296875, 0.74609375, -0.59375, 0.609375, -0.5703125, 0.859375, -0.12451171875, 0.034423828125, -0.255859375, -0.1484375, -0.294921875, -0.046630859375, 0.07568359375, -0.2890625, -0.5078125, 0.1220703125, -0.37890625, 0.06201171875, -0.240234375, -0.08203125, -0.2109375, -0.11474609375, 0.0086669921875, 0.00994873046875, 0.64453125, 0.052001953125, 0.5546875, 0.21875, 0.20703125, -0.140625, -0.76171875, 0.63671875, 0.8984375, -0.6953125, -0.43359375, 0.09765625, 0.361328125, -0.546875, 0.1884765625, -0.068359375, 0.00732421875, 0.0262451171875, -0.1669921875, -0.1318359375, 0.052978515625, 0.328125, -0.064453125, -0.369140625, 0.478515625, -0.11083984375, -0.73046875, 0.32421875, -0.01611328125, 0.041748046875, -0.09326171875, 0.09765625, 0.03271484375, 0.81640625, 0.0311279296875, -0.1611328125, 0.2158203125, -0.5390625, -0.33984375, 0.004058837890625, -0.1484375, -0.0250244140625, 0.578125, -0.171875, 0.1611328125, -0.5, -0.34375, 0.2109375, -0.11083984375, 0.490234375, 0.37109375, -0.162109375, 0.61328125, -0.1416015625, -0.25390625, 0.10302734375, 0.404296875, -0.0673828125, 0.240234375, 0.3359375, 0.2001953125, 0.48046875, 0.6796875, 0.0230712890625, 0.4609375, -0.125, -0.10986328125, -0.80078125, 0.431640625, -0.1943359375, -0.224609375, 0.48046875, -0.150390625, -0.020263671875, -0.01904296875, 0.2451171875, 0.1298828125, 0.859375, -0.60546875, 0.6796875, 0.0255126953125, -1.0390625, 0.75, 0.423828125, 0.244140625, 0.17578125, 0.2734375, 0.32421875, 0.01092529296875, 0.26171875, 0.162109375, -0.337890625, -0.474609375, -0.357421875, 0.2314453125, -0.04833984375, -0.1455078125, 0.671875, -0.5859375, -0.275390625, -0.06396484375, 0.01300048828125, -0.490234375, -0.546875, 0.13671875, -0.443359375, 0.3359375, 0.80078125, -0.23828125, -0.054931640625, -0.5234375, -0.0284423828125, -0.0255126953125, 0.10693359375, -0.0810546875, -0.1435546875, 0.04541015625, -0.671875, 0.357421875, -0.349609375, 0.2353515625, 0.1181640625, -0.39453125, -0.7578125, 0.6171875, -0.000812530517578125, 0.1826171875, -0.003814697265625, -0.123046875, -0.345703125, 0.69921875, -0.1103515625, -0.06494140625, 0.25, -0.09130859375, -0.039794921875, -0.125, -0.44921875, 0.004730224609375, 0.1494140625, -0.061279296875, 0.3203125, -0.2890625, -0.1396484375, 0.419921875, -0.314453125, -0.427734375, -0.5546875, -0.279296875, 0.328125, -0.05517578125, -0.44921875, -0.703125, 0.130859375, -0.2353515625, -0.5234375, -0.09912109375, -0.703125, 0.54296875, -0.68359375, -0.51953125, 0.65234375, -0.330078125, 0.10986328125, 0.000316619873046875, -0.69140625, -0.06982421875, -0.15625, 0.216796875, -0.53125, -0.828125, 0.197265625, 0.50390625, -0.06884765625, 0.78515625, 0.625, 0.048095703125, -0.11328125, 0.3984375, -0.0400390625, -0.384765625, -0.2138671875, -0.3125, -0.06689453125, -0.33984375, -0.3984375, 0.1650390625, 0.07275390625, -0.4296875, 0.033935546875, -0.002716064453125, -0.2890625, 0.1572265625, -0.546875, 0.0184326171875, 0.49609375, -0.162109375, -0.431640625, -0.76171875, 0.640625, -0.1103515625, 0.75, -0.03369140625, 0.111328125, 0.142578125, 0.5859375, 0.39453125, -0.1748046875, 0.7734375, 0.69921875, 0.58203125, -0.54296875, -0.462890625, 0.15234375, 0.08349609375, 0.318359375, -1.0625, 0.006439208984375, -0.07373046875, -0.1220703125, 0.71875, 0.26953125, 0.47265625, 0.039306640625, -0.0595703125, 0.130859375, -0.138671875, -0.474609375, -0.2734375, -0.47265625, -0.859375, 0.34375, 1.1015625, 0.10986328125, -0.0537109375, 0.43359375, 0.060791015625, -0.27734375, -0.51171875, -0.515625, 0.2021484375, -0.359375, 0.13671875, 0.251953125, -0.33984375, 0.033447265625, -0.1064453125, -0.322265625, 0.67578125, 0.294921875, 0.7578125, -0.322265625, 0.09033203125, 0.2578125, 0.1484375, -0.3359375, 0.32421875, -0.69921875, 0.67578125, 0.1796875, -0.10009765625, -0.251953125, -0.287109375, 0.388671875, -0.380859375, -0.47265625, 0.44140625, 0.6015625, 0.94921875, -0.302734375, -0.1943359375, -0.94140625, -0.40625, 0.095703125, -0.53515625, -0.224609375, 0.30078125, 0.0098876953125, 0.1865234375, 0.64453125, -0.53125, -0.61328125, -0.34375, -0.55078125, 0.14453125, -0.1865234375, -0.1640625, 0.1826171875, 0.026123046875, -1.1796875, 0.1591796875, -0.8046875, 0.24609375, -0.427734375, -1.125, 0.173828125, 0.07421875, -0.23828125, 0.71484375, -0.0380859375, -0.08154296875, 0.37109375, 0.1123046875, -0.421875, -0.0120849609375, -0.2197265625, 0.064453125, 0.6640625, -0.51953125, 0.578125, -0.462890625, 0.41796875, -0.146484375, -0.90625, -0.61328125, 1.5078125, -0.2080078125, -0.423828125, -0.57421875, -0.0184326171875, -0.50390625, 0.01318359375, -0.46484375, -0.09130859375, -0.18359375, -0.043701171875, -0.44921875, -0.294921875, -1.5234375, 0.5625, -0.326171875, -0.431640625, 0.154296875, -0.0189208984375, -0.375, -0.263671875, 0.0011749267578125, 0.014892578125, -0.12353515625, 1.0390625, 0.130859375, 0.71875, 0.8203125, -0.287109375, 0.4140625, 0.19140625, 0.291015625, 0.19921875, 0.94140625, 0.44140625, 0.45703125, -0.427734375, 0.29296875, -0.0123291015625, -0.2470703125, 0.515625, -0.462890625, -0.455078125, 0.9921875, 0.1953125, -0.002899169921875, 0.447265625, -0.51953125, -0.306640625, 0.703125, -0.13671875, -0.265625, 0.1806640625, -0.94921875, 0.1728515625, 0.177734375, 0.091796875, 0.134765625, 0.98828125, 0.41015625, -0.3203125, -0.75, -0.60546875, 0.283203125, 0.007720947265625, -0.208984375, -0.1044921875, -0.2294921875, 0.1220703125, -0.40234375, 0.11083984375, -0.302734375, -0.34765625, 0.12451171875, 0.07861328125, 0.205078125, 0.74609375, 0.0478515625, 1.0, 0.0869140625, 0.26953125, -0.21484375, 0.69921875, -0.3046875]\n",
      "========================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StreamingResponse' object has no attribute 'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m)\n\u001b[1;32m      8\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StreamingResponse' object has no attribute 'response'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = query_engine.aquery(\"Quelle norme de tube acier utiliser pour faire une liaison détente / comptage pour alimenter une chaufferie ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6466ee54-5ee3-4cd4-a8c8-9814fb2e9154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'88d669e7-1330-4eef-a1c1-edf9f994d522': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'DDC'},\n",
       " 'fee16e37-172c-4353-aab3-2210390056af': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': '6M'},\n",
       " '2ba547ac-8e18-4a65-87c6-e243d5968e8d': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'JM'},\n",
       " '7c6a7785-2cbb-4cd9-8bd8-5cf375e4416c': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'AMR'},\n",
       " '78b03ec1-4a24-4f74-acc6-77698a24f7f9': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'JJ'},\n",
       " '0eecb069-97fe-48f5-9ae7-69cdbf076a22': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'CPT'},\n",
       " '3d22013f-3375-429d-8b48-7e2da5760852': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'PCGR'},\n",
       " '2970fd28-8d78-4b00-a3b8-2c45527336ca': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'POC'},\n",
       " 'bdca1a4c-8842-47aa-a36f-a91829def5d3': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'PCE'},\n",
       " '7995a02e-67ec-48dd-ac06-79afee7e1361': {'file_path': 'Lexique.csv',\n",
       "  'file_name': 'Lexique.csv',\n",
       "  'file_type': '.csv',\n",
       "  'file_size': '180605',\n",
       "  'creation_date': '2024-03-06',\n",
       "  'last_modified_date': '2024-03-06 10:14:03',\n",
       "  'last_accessed_date': '2024-03-07 13:37:50',\n",
       "  'catégorie': 'vocabulaire',\n",
       "  'keyword': 'AMM'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "89876619-0371-439e-9aab-a2726ac83d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Reranked nodes are extracted.\n",
      "Step reranker_prompt produced event StopEvent\n",
      "========================================\n",
      "Les informations contextuelles ne mentionnent pas d'achats immobilisés pour le biométhane. Cependant, elles fournissent des détails sur les éléments suivants :\n",
      "\n",
      "1- Le périmètre des prestations de maintenance des postes d'injection de biométhane, qui ne comprend pas l'exploitation des postes mais permet à GRDF de reprendre la maintenance préventive et corrective de certains équipements.\n",
      "\n",
      "2- Les exigences techniques relatives aux interventions sur ouvrages en exploitation, qui font l'objet d'un cahier des clauses techniques particulières spécifiques.\n",
      "\n",
      "3- Les conditions de passage d'une \"maintenance 100% Fournisseur\" à une \"maintenance partagée\", qui sont listées dans le document.\n",
      "\n",
      "4- L'obligation de réversibilité de la prestation de maintenance, dans le cas où GRDF déciderait d'ouvrir le marché à d'autres prestataires que les constructeurs de postes.\n",
      "\n",
      "5- Les éléments à inclure dans les bilans annuels de maintenance, tels que la typologie des pannes, les modes opératoires de résolution et les prestations d'éventuels sous-traitants.\n",
      "========================================\n",
      "Execution Time: 11.132328987121582 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Quels sont les achats immobilisés pour le biométhane ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70b67184-f504-4090-8dea-6b7643127afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Reranked nodes are extracted.\n",
      "Step reranker_prompt produced event StopEvent\n",
      "========================================\n",
      "CICM signifie \"Conduite d'Immeuble, Conduite Montante\". C'est un terme utilisé pour désigner les canalisations de gaz qui alimentent les immeubles collectifs, notamment la conduite d'immeuble et la conduite montante.\n",
      "========================================\n",
      "Execution Time: 8.808188676834106 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Que veut dire CICM ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1132a9f8-a221-4c6c-916f-266a4bfe6c20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async activated.\n",
      "Running step sub_question\n",
      "Step sub_question produced event SubQuestionEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Step rerank_validate produced event RerankerBatchEvent\n",
      "Running step reranker_prompt\n",
      "Step reranker_prompt produced event RerankerDone\n",
      "Running step rerank_validate\n",
      "Reranked nodes are extracted.\n",
      "Step rerank_validate produced event RerankerValidationDone\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event FilterDone\n",
      "Running step filter_validate\n",
      "Validation failed, retrying...\n",
      "Step filter_validate produced event FilterValidationErrorEvent\n",
      "Running step filter_prompt\n",
      "Step filter_prompt produced event StopEvent\n",
      "========================================\n",
      "Oui, la surveillance des réseaux doit inclure la surveillance des branchements. Les branchements représentent en effet 75% des dommages aux ouvrages. La réglementation technique applicable aux réseaux de distribution dispose que l'opérateur de réseau gaz doit réaliser la surveillance des réseaux selon des procédures préétablies, documentées et systématiques. Cette surveillance doit porter sur l'ensemble des ouvrages composant le réseau, y compris les branchements.\n",
      "\n",
      "Plus spécifiquement :\n",
      "\n",
      "1- La surveillance des branchements doit être réalisée sur les parties enterrées et aériennes, au droit des regards ou coffrets enterrés, ainsi qu'au droit des coffrets hors sol sans ouverture de ces derniers.\n",
      "2- Pour les immeubles collectifs, la surveillance doit intégrer les conduites d'immeuble enterrées en aval de l'Organe de Coupure Général avant pénétration.\n",
      "3- La détection se fait le plus près possible de la canalisation, les voies canalisées des deux côtés faisant l'objet d'une surveillance des deux côtés de la voie.\n",
      "========================================\n",
      "Execution Time: 10.47767949104309 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "response = await query_engine.aquery(\"Dans le cadre de la surveillance des réseaux, doit-on surveiller les branchements ?\")\n",
    "print(\"========================================\")\n",
    "print(response.response)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"========================================\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66a749-0dfc-4754-9faf-233a9e6d8bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
